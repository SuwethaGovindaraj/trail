{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------------------------\n",
    "# Load your data\n",
    "# --------------------------\n",
    "# Replace 'claims.csv' with your file path\n",
    "df = pd.read_csv('claims.csv')\n",
    "\n",
    "# Merge notes\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" +\n",
    "    df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning\n",
    "# --------------------------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Vectorize & cluster\n",
    "# --------------------------\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# Choose cluster count (tweak as needed)\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "df['cluster'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Top terms per cluster\n",
    "# --------------------------\n",
    "import numpy as np\n",
    "\n",
    "def top_terms_for_cluster(cluster_id, top_n=15):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    return [(terms[i], mean_tfidf[i]) for i in top_idx]\n",
    "\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} (size {len(df[df['cluster']==c])}):\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"{term:20s} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Add evidence column: matched top terms from each note's cluster\n",
    "# --------------------------\n",
    "def find_matches_in_text(text, terms):\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', text):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "# Build dictionary of cluster -> top terms\n",
    "cluster_top_terms = {c: [t for t, _ in top_terms_for_cluster(c)] for c in range(n_clusters)}\n",
    "\n",
    "# Create column with matches\n",
    "df[\"matched_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"clean_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df[[\"merged_text\", \"cluster\", \"matched_terms\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Show sample notes per cluster\n",
    "# --------------------------\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(SAMPLES_PER_CLUSTER)\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Interactive cluster tagging\n",
    "# --------------------------\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top terms:\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"  {term:20s} {score:.4f}\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(\"Enter label for this cluster (finding/nofinding/cancelled/unknown): \").strip().lower()\n",
    "    cluster_labels[c] = label\n",
    "\n",
    "# Assign labels back to df\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "# Save labeled dataset\n",
    "df.to_csv('claims_cluster_tagged.csv', index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456de5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning function\n",
    "# --------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove IDs like n19878, abc123 (letters+digits combos)\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\d{3,}\\b', ' ', text)\n",
    "    # Remove years/numbers\n",
    "    text = re.sub(r'\\b\\d{2,4}\\b', ' ', text)\n",
    "    # Remove known meaningless codes (expand this list as you see them)\n",
    "    custom_stopwords = {'drg', 'hrp', 'etc', 'na'}\n",
    "    text = ' '.join(w for w in text.split() if w not in custom_stopwords)\n",
    "    # Remove special characters & extra spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --------------------------\n",
    "# Prepare data\n",
    "# --------------------------\n",
    "df['merged_text'] = (\n",
    "    df['communication_notes'].fillna('') + ' ' + df['free_flow_opt_note'].fillna('')\n",
    ").apply(clean_text)\n",
    "\n",
    "# --------------------------\n",
    "# TF-IDF with bigrams\n",
    "# --------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=0.85,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2)  # single words + 2-word phrases\n",
    ")\n",
    "X = vectorizer.fit_transform(df['merged_text'])\n",
    "\n",
    "# --------------------------\n",
    "# Cluster\n",
    "# --------------------------\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# --------------------------\n",
    "# Top terms per cluster\n",
    "# --------------------------\n",
    "def top_terms_for_cluster(cluster_id, top_n=15, min_score=0.01):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1]\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        if mean_tfidf[i] >= min_score:  # skip very low-weight terms\n",
    "            results.append((terms[i], mean_tfidf[i]))\n",
    "        if len(results) >= top_n:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "# --------------------------\n",
    "# Display results\n",
    "# --------------------------\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n{'='*50}\\nCluster {c} (size {len(df[df['cluster']==c])})\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"{term:25s} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02016215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning function\n",
    "# --------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\d{3,}\\b', ' ', text)   # IDs\n",
    "    text = re.sub(r'\\b\\d{2,4}\\b', ' ', text)            # years/numbers\n",
    "    custom_stopwords = {'drg', 'hrp', 'etc', 'na'}      # expand this\n",
    "    text = ' '.join(w for w in text.split() if w not in custom_stopwords)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)               # non-letters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --------------------------\n",
    "# Prepare data\n",
    "# --------------------------\n",
    "df['merged_text'] = (\n",
    "    df['communication_notes'].fillna('') + ' ' + df['free_flow_opt_note'].fillna('')\n",
    ").apply(clean_text)\n",
    "\n",
    "# --------------------------\n",
    "# TF-IDF with bigrams\n",
    "# --------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=0.85,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "X = vectorizer.fit_transform(df['merged_text'])\n",
    "\n",
    "# --------------------------\n",
    "# Cluster\n",
    "# --------------------------\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# --------------------------\n",
    "# Top terms per cluster\n",
    "# --------------------------\n",
    "def top_terms_for_cluster(cluster_id, top_n=15, min_score=0.01):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1]\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        if mean_tfidf[i] >= min_score:\n",
    "            results.append((terms[i], mean_tfidf[i]))\n",
    "        if len(results) >= top_n:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "# --------------------------\n",
    "# Add evidence column\n",
    "# --------------------------\n",
    "def find_matches_in_text(text, terms):\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', text):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "cluster_top_terms = {c: [t for t, _ in top_terms_for_cluster(c)] for c in range(n_clusters)}\n",
    "df[\"matched_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"merged_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Show sample notes per cluster\n",
    "# --------------------------\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(SAMPLES_PER_CLUSTER)\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "\n",
    "# --------------------------\n",
    "# Interactive cluster tagging\n",
    "# --------------------------\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top terms:\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"  {term:20s} {score:.4f}\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(\"Enter label for this cluster (finding/nofinding/cancelled/unknown): \").strip().lower()\n",
    "    cluster_labels[c] = label\n",
    "\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "df.to_csv('claims_cluster_tagged.csv', index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Imports\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# ============================\n",
    "# 2. Load your data\n",
    "# ============================\n",
    "# df = pd.read_csv(\"your_claims.csv\")  # Replace with your file\n",
    "# Example columns: communication_notes, free_flow_opt_note\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\").astype(str) + \" \" +\n",
    "    df[\"free_flow_opt_note\"].fillna(\"\").astype(str)\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. Clean text\n",
    "# ============================\n",
    "def clean_text(txt):\n",
    "    txt = str(txt).lower()\n",
    "    txt = re.sub(r\"\\bnan\\b\", \" \", txt)             # remove 'nan'\n",
    "    txt = re.sub(r\"[^a-z0-9\\s]\", \" \", txt)         # remove punctuation\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "df[\"clean_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# ============================\n",
    "# 4. TF-IDF with bigrams & trigrams\n",
    "# ============================\n",
    "# Keep default stopwords but allow important domain words\n",
    "custom_stopwords = text.ENGLISH_STOP_WORDS - {\n",
    "    \"letter\", \"request\", \"sent\", \"medical\", \"record\"\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stopwords,\n",
    "    ngram_range=(1, 3),        # unigrams, bigrams, trigrams\n",
    "    min_df=5                   # must appear in at least 5 docs\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# ============================\n",
    "# 5. KMeans clustering\n",
    "# ============================\n",
    "n_clusters = 5  # You can adjust this\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# ============================\n",
    "# 6. Function: Top phrases per cluster\n",
    "# ============================\n",
    "def top_terms_for_cluster(cluster_id, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    return [(terms[i], mean_tfidf[i]) for i in top_idx]\n",
    "\n",
    "# Show top terms per cluster\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} (size {len(df[df['cluster'] == c])}) ===\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"{term:40s} {score:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 7. Evidence phrase extraction\n",
    "# ============================\n",
    "def find_matches_in_text(text, terms):\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        # Match full term as phrase\n",
    "        if re.search(r\"\\b\" + re.escape(term) + r\"\\b\", text):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "# Dictionary: cluster â†’ top phrases\n",
    "cluster_top_terms = {\n",
    "    c: [t for t, _ in top_terms_for_cluster(c)]\n",
    "    for c in range(n_clusters)\n",
    "}\n",
    "\n",
    "# Create column with matched phrases\n",
    "df[\"matched_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"clean_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 8. Sample viewer per cluster\n",
    "# ============================\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    print(\"Top phrases:\", \", \".join(cluster_top_terms[c]))\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].sample(min(SAMPLES_PER_CLUSTER, len(df[df['cluster'] == c])))\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "\n",
    "# ============================\n",
    "# 9. Interactive labeling\n",
    "# ============================\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top phrases: {', '.join(cluster_top_terms[c][:10])}\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(\"Enter label for this cluster (finding/nofinding/cancelled/unknown): \").strip().lower()\n",
    "    cluster_labels[c] = label\n",
    "\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "# ============================\n",
    "# 10. Save\n",
    "# ============================\n",
    "df.to_csv(\"claims_cluster_tagged.csv\", index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08843a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Imports\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# ============================\n",
    "# 2. Load your data\n",
    "# ============================\n",
    "# df = pd.read_csv(\"your_claims.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\").astype(str) + \" \" +\n",
    "    df[\"free_flow_opt_note\"].fillna(\"\").astype(str)\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. Clean text\n",
    "# ============================\n",
    "def clean_text(txt):\n",
    "    txt = str(txt).lower()\n",
    "    txt = re.sub(r\"\\bnan\\b\", \" \", txt)  # remove 'nan'\n",
    "    txt = re.sub(r\"[^a-z0-9\\s]\", \" \", txt)  # remove punctuation\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "df[\"clean_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# ============================\n",
    "# 4. TF-IDF with bigrams/trigrams\n",
    "# ============================\n",
    "custom_stopwords = text.ENGLISH_STOP_WORDS - {\n",
    "    \"letter\", \"request\", \"sent\", \"medical\", \"record\"\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stopwords,\n",
    "    ngram_range=(1, 3),        # unigrams, bigrams, trigrams\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# ============================\n",
    "# 5. KMeans clustering\n",
    "# ============================\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# ============================\n",
    "# 6. Top phrases per cluster\n",
    "# ============================\n",
    "def top_terms_for_cluster(cluster_id, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    return [(terms[i], mean_tfidf[i]) for i in top_idx]\n",
    "\n",
    "cluster_top_terms = {\n",
    "    c: [t for t, _ in top_terms_for_cluster(c)]\n",
    "    for c in range(n_clusters)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# 7. Evidence phrase extraction\n",
    "# ============================\n",
    "def find_matches_in_text(text, terms):\n",
    "    return [term for term in terms if re.search(r\"\\b\" + re.escape(term) + r\"\\b\", text)]\n",
    "\n",
    "df[\"matched_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"clean_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 8. Auto-suggest cluster labels\n",
    "# ============================\n",
    "FINDING_KEYWORDS = [\n",
    "    \"overpayment\", \"offset\", \"uncheck\", \"refund\", \"rpa\",\n",
    "    \"recoupment\", \"dispute\", \"recover\", \"repayment\"\n",
    "]\n",
    "NOFINDING_KEYWORDS = [\n",
    "    \"no findings\", \"no issue\", \"valid\", \"compliant\",\n",
    "    \"nothing found\", \"no error\"\n",
    "]\n",
    "CANCELLED_KEYWORDS = [\n",
    "    \"cancelled\", \"withdrawn\", \"closed without action\",\n",
    "    \"claim withdrawn\"\n",
    "]\n",
    "\n",
    "def auto_label_cluster(phrases):\n",
    "    joined = \" \".join(phrases).lower()\n",
    "    if any(kw in joined for kw in FINDING_KEYWORDS):\n",
    "        return \"finding\"\n",
    "    elif any(kw in joined for kw in NOFINDING_KEYWORDS):\n",
    "        return \"nofinding\"\n",
    "    elif any(kw in joined for kw in CANCELLED_KEYWORDS):\n",
    "        return \"cancelled\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    suggestion = auto_label_cluster(cluster_top_terms[c])\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    print(\"Top phrases:\", \", \".join(cluster_top_terms[c]))\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(f\"Suggested label: {suggestion} â€” Press Enter to accept or type new label: \").strip().lower()\n",
    "    cluster_labels[c] = label if label else suggestion\n",
    "\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "# ============================\n",
    "# 9. Save\n",
    "# ============================\n",
    "df.to_csv(\"claims_cluster_tagged.csv\", index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 1. Imports\n",
    "# =====================================\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# =====================================\n",
    "# 2. Load your dataset\n",
    "# =====================================\n",
    "# df = pd.read_csv(\"your_claims_data.csv\")  # Replace with your file path\n",
    "\n",
    "# Merge the relevant text columns into one\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# Basic cleaning: lowercase, remove special chars\n",
    "df[\"clean_text\"] = (\n",
    "    df[\"merged_text\"]\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9\\s]\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# =====================================\n",
    "# 3. Remove stopwords, codes, numbers\n",
    "# =====================================\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"ovptrxid\", \"nan\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}\n",
    "\n",
    "def clean_and_filter_tokens(text):\n",
    "    tokens = text.split()\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in REMOVE_TERMS:\n",
    "            continue\n",
    "        if token.isdigit():\n",
    "            continue\n",
    "        if len(token) < MIN_WORD_LEN and token.lower() not in WHITELIST:\n",
    "            continue\n",
    "        filtered.append(token)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"clean_text\"].apply(clean_and_filter_tokens)\n",
    "\n",
    "# =====================================\n",
    "# 4. TF-IDF Vectorization\n",
    "# =====================================\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),  # unigrams, bigrams, trigrams\n",
    "    min_df=5,            # drop rare terms\n",
    "    token_pattern=r'\\b[a-zA-Z][a-zA-Z]+\\b'\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "\n",
    "# =====================================\n",
    "# 5. Clustering\n",
    "# =====================================\n",
    "n_clusters = 5  # You can adjust\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# =====================================\n",
    "# 6. Function to get top terms per cluster\n",
    "# =====================================\n",
    "def top_terms_for_cluster(cluster_id, top_n=15):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    return [(terms[i], mean_tfidf[i]) for i in top_idx]\n",
    "\n",
    "# =====================================\n",
    "# 7. Print top terms\n",
    "# =====================================\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} (size {len(df[df['cluster']==c])}):\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"{term:40s} {score:.4f}\")\n",
    "\n",
    "# =====================================\n",
    "# 8. Add evidence_terms column\n",
    "# =====================================\n",
    "def find_matches_in_text(text, terms):\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', text):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "cluster_top_terms = {c: [t for t, _ in top_terms_for_cluster(c)] for c in range(n_clusters)}\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"filtered_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# =====================================\n",
    "# 9. Show sample notes per cluster\n",
    "# =====================================\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(SAMPLES_PER_CLUSTER)\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "\n",
    "# =====================================\n",
    "# 10. Interactive cluster tagging\n",
    "# =====================================\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top terms:\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"  {term:40s} {score:.4f}\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(\"Enter label for this cluster (finding/nofinding/cancelled/unknown): \").strip().lower()\n",
    "    cluster_labels[c] = label\n",
    "\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "# =====================================\n",
    "# 11. Save final labeled dataset\n",
    "# =====================================\n",
    "df.to_csv('claims_cluster_tagged.csv', index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ef97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# === Step 1: Clean and remove generic terms ===\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"nan\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # remove numbers/special chars\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# === Step 2: Vectorize with n-grams ===\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# === Step 3: Cluster ===\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# === Step 4: Top terms per cluster (remove overlaps) ===\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    return remove_redundant_phrases(raw_terms)\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(n_clusters)}\n",
    "\n",
    "# === Step 5: Evidence column ===\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Step 6: Rule-based label suggestion ===\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_set = set(evidence)\n",
    "    if any(p in ev_set for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_set for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_set for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)), axis=1\n",
    ")\n",
    "\n",
    "# === Step 7: Save results ===\n",
    "df.to_csv(\"claims_with_labels.csv\", index=False)\n",
    "print(\"Saved to claims_with_labels.csv\")\n",
    "\n",
    "# === Debug: Show clusters ===\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c}: {cluster_terms[c]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70039a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Load & merge text columns\n",
    "# =========================\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 1: Clean text\n",
    "# =========================\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"nan\", \"request\", \"letter\", \"sent\", \"medical\", \"record\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}  # short words you still want\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # keep only letters\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# =========================\n",
    "# Step 2: TF-IDF Vectorizer (1-3 grams)\n",
    "# =========================\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# =========================\n",
    "# Step 3: KMeans clustering\n",
    "# =========================\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# =========================\n",
    "# Step 4: Top terms per cluster (remove overlaps)\n",
    "# =========================\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    return remove_redundant_phrases(raw_terms)\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(n_clusters)}\n",
    "\n",
    "# =========================\n",
    "# Step 5: Evidence terms per claim\n",
    "# =========================\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 6: Rule-based label suggestion\n",
    "# =========================\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_set = set(evidence)\n",
    "    if any(p in ev_set for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_set for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_set for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)), axis=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 7: Save results\n",
    "# =========================\n",
    "df.to_csv(\"claims_with_labels.csv\", index=False)\n",
    "print(\"âœ… Saved to claims_with_labels.csv\")\n",
    "\n",
    "# =========================\n",
    "# Step 8: Show cluster summaries\n",
    "# =========================\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "\n",
    "for c in range(n_clusters):\n",
    "    cluster_df = df[df['cluster'] == c]\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸ“Œ Cluster {c} â€” size: {len(cluster_df)}\")\n",
    "    print(\"Top 20 terms/phrases:\")\n",
    "    for term in cluster_terms[c]:\n",
    "        print(f\"  {term}\")\n",
    "\n",
    "    # Overpayment stats\n",
    "    if \"overpayment_amount\" in df.columns:\n",
    "        pos_amount = (cluster_df[\"overpayment_amount\"] > 0).mean() * 100\n",
    "        print(f\"\\nðŸ’° % with positive overpayment_amount: {pos_amount:.1f}%\")\n",
    "\n",
    "    print(\"\\nSample claim notes:\")\n",
    "    sample_rows = cluster_df.sample(\n",
    "        min(SAMPLES_PER_CLUSTER, len(cluster_df)),\n",
    "        random_state=42\n",
    "    )\n",
    "    for note in sample_rows[\"merged_text\"]:\n",
    "        print(f\"- {note}\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 1: Clean text\n",
    "# =========================\n",
    "REMOVE_TERMS = {\n",
    "    \"md\", \"drg\", \"hrp\", \"nan\", \"request\", \"letter\", \"sent\", \"medical\", \"record\",\n",
    "    \"chart\", \"order\", \"verify\", \"reported\", \"please\", \"note\", \"per\"\n",
    "}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}  # short words you want to keep\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# =========================\n",
    "# Step 2: TF-IDF Vectorizer (1-3 grams)\n",
    "# =========================\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5,\n",
    "    max_df=0.8  # remove phrases appearing in >80% of claims\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# =========================\n",
    "# Step 3: KMeans clustering\n",
    "# =========================\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# =========================\n",
    "# Step 4: Top terms per cluster (remove overlaps)\n",
    "# =========================\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n * 2]  # take more, then filter\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    # remove admin-like meaningless phrases again\n",
    "    cleaned = [t for t in raw_terms if not any(stop in t.split() for stop in REMOVE_TERMS)]\n",
    "    return remove_redundant_phrases(cleaned)[:top_n]\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(n_clusters)}\n",
    "\n",
    "# =========================\n",
    "# Step 5: Evidence terms per claim\n",
    "# =========================\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 6: Rule-based label suggestion\n",
    "# =========================\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_set = set(evidence)\n",
    "    if any(p in ev_set for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_set for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_set for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)), axis=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 7: Create final descriptive column for clarity\n",
    "# =========================\n",
    "df[\"cluster_top_terms\"] = df[\"cluster\"].apply(lambda c: \", \".join(cluster_terms[c]))\n",
    "\n",
    "# =========================\n",
    "# Step 8: Save full enriched dataset\n",
    "# =========================\n",
    "df.to_csv(\"claims_with_clusters_and_labels.csv\", index=False)\n",
    "print(\"âœ… Saved to claims_with_clusters_and_labels.csv\")\n",
    "\n",
    "# =========================\n",
    "# Step 9: Show summaries\n",
    "# =========================\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "\n",
    "for c in range(n_clusters):\n",
    "    cluster_df = df[df['cluster'] == c]\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸ“Œ Cluster {c} â€” size: {len(cluster_df)}\")\n",
    "    print(\"Top 20 terms/phrases:\")\n",
    "    for term in cluster_terms[c]:\n",
    "        print(f\"  {term}\")\n",
    "\n",
    "    if \"overpayment_amount\" in df.columns:\n",
    "        pos_amount = (cluster_df[\"overpayment_amount\"] > 0).mean() * 100\n",
    "        print(f\"\\nðŸ’° % with positive overpayment_amount: {pos_amount:.1f}%\")\n",
    "\n",
    "    print(\"\\nSample claim notes:\")\n",
    "    sample_rows = cluster_df.sample(\n",
    "        min(SAMPLES_PER_CLUSTER, len(cluster_df)),\n",
    "        random_state=42\n",
    "    )\n",
    "    for note in sample_rows[\"merged_text\"]:\n",
    "        print(f\"- {note}\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52620e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load data\n",
    "# ----------------------------------------\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 1: Clean and remove generic terms\n",
    "# ----------------------------------------\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"nan\", \"request\", \"letter\", \"sent\", \"medical\", \"record\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # remove numbers/special chars\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 2: Vectorize with n-grams\n",
    "# ----------------------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 3: Cluster\n",
    "# ----------------------------------------\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 4: Top terms per cluster (remove overlaps)\n",
    "# ----------------------------------------\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    return remove_redundant_phrases(raw_terms)\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(n_clusters)}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 5: Evidence column\n",
    "# ----------------------------------------\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 6: Rule-based label suggestion\n",
    "# ----------------------------------------\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_set = set(evidence)\n",
    "    if any(p in ev_set for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_set for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_set for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)), axis=1\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 7: Save results\n",
    "# ----------------------------------------\n",
    "df.to_csv(\"claims_with_labels.csv\", index=False)\n",
    "print(\"Saved to claims_with_labels.csv\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 8: Debug - Show clusters\n",
    "# ----------------------------------------\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c}: {cluster_terms[c]}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# EXTRA VIEW: Filter top terms by minimum TF-IDF weight\n",
    "# ----------------------------------------\n",
    "MIN_TFIDF_WEIGHT = 0.05  # adjust threshold\n",
    "\n",
    "def top_terms_above_threshold(c, min_weight=MIN_TFIDF_WEIGHT, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    mask = mean_tfidf >= min_weight\n",
    "    filtered_idx = np.where(mask)[0]\n",
    "    sorted_idx = filtered_idx[np.argsort(mean_tfidf[filtered_idx])[::-1]]\n",
    "    raw_terms = [terms[i] for i in sorted_idx]\n",
    "    cleaned = [t for t in raw_terms if not any(stop in t.split() for stop in REMOVE_TERMS)]\n",
    "    return remove_redundant_phrases(cleaned)[:top_n]\n",
    "\n",
    "strong_cluster_terms = {c: top_terms_above_threshold(c) for c in range(n_clusters)}\n",
    "\n",
    "print(\"\\n\\nðŸ“Š Strong Signal Top Terms per Cluster (TF-IDF >= {:.2f})\".format(MIN_TFIDF_WEIGHT))\n",
    "for c in range(n_clusters):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Cluster {c}\")\n",
    "    for term in strong_cluster_terms[c]:\n",
    "        print(f\"  {term}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 9: Add strong terms column for each row & save\n",
    "# ----------------------------------------\n",
    "df[\"strong_terms\"] = df[\"cluster\"].map(strong_cluster_terms)\n",
    "\n",
    "# Final export with everything\n",
    "df.to_csv(\"claims_with_labels_and_strong_terms.csv\", index=False)\n",
    "print(\"Saved to claims_with_labels_and_strong_terms.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95614a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full improved pipeline: clustering + phrase mining + weak-supervision + classifier + evidence\n",
    "# Requirements: pandas, numpy, scikit-learn\n",
    "# Optional but recommended: increase min_df if you have extremely large data\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration (tune here)\n",
    "# ---------------------------\n",
    "N_CLUSTERS = 5               # clustering (kept for exploration)\n",
    "TF_MIN_DF = 8                # minimum docs an ngram must appear in\n",
    "TF_MAX_DF = 0.85             # remove very common phrases\n",
    "TF_NGRAM_RANGE = (1, 3)      # unigrams..trigrams\n",
    "TF_MAX_FEATURES = 20000      # limit vocabulary\n",
    "CHI2_TOP_K = 300             # candidate ngrams to treat as 'finding' seeds\n",
    "MIN_CHI2_SCORE = 10.0        # filter weak chi2 signals\n",
    "SEED_MIN_COUNT = 20          # only use ngrams that appear at least this often for seeds\n",
    "PROB_THRESHOLD = 0.80        # assign final label only when model is confident\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ---------------------------\n",
    "# Load your data (edit)\n",
    "# ---------------------------\n",
    "# df = pd.read_csv(\"claims_data.csv\")       # uncomment and set path\n",
    "# For this snippet we assume df exists and has:\n",
    "# - communication_notes\n",
    "# - free_flow_opt_note\n",
    "# - overpayment_amount\n",
    "\n",
    "# ---------------------------\n",
    "# Merge + basic cleaning\n",
    "# ---------------------------\n",
    "def preprocess_text(s):\n",
    "    if pd.isna(s) or s is None:\n",
    "        return \"\"\n",
    "    text = str(s).lower()\n",
    "    # remove emails/urls/phone-like tokens, claim ids (like n19878) and numeric tokens\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\d{2,}\\b', ' ', text)   # small letter+digits patterns\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    # keep letters and spaces (we will later drop many short tokens)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['merged_text'] = (df['communication_notes'].fillna('') + ' ' +\n",
    "                     df['free_flow_opt_note'].fillna('')).apply(preprocess_text)\n",
    "\n",
    "# ---------------------------\n",
    "# Token-level filtering (remove short noisy tokens, common codes)\n",
    "# ---------------------------\n",
    "NOISE_TOKENS = set([\n",
    "    'nan','md','drg','hrp','ovptrxid','omniclaim','claim','facility','provider','sent',\n",
    "    'request','requested','please','note','order','chart','record','verify','reported'\n",
    "])\n",
    "MIN_TOKEN_LEN = 3\n",
    "WHITELIST_SHORT = {'rpa','offset'}  # keep these short tokens if present\n",
    "\n",
    "def filter_tokens(text):\n",
    "    toks = []\n",
    "    for t in text.split():\n",
    "        if t in NOISE_TOKENS:\n",
    "            continue\n",
    "        if len(t) < MIN_TOKEN_LEN and t not in WHITELIST_SHORT:\n",
    "            continue\n",
    "        toks.append(t)\n",
    "    return ' '.join(toks)\n",
    "\n",
    "df['filtered_text'] = df['merged_text'].apply(filter_tokens)\n",
    "\n",
    "# ---------------------------\n",
    "# Vectorization (TF-IDF for classifier + Count for chi2)\n",
    "# ---------------------------\n",
    "tfvec = TfidfVectorizer(stop_words='english',\n",
    "                        ngram_range=TF_NGRAM_RANGE,\n",
    "                        min_df=TF_MIN_DF,\n",
    "                        max_df=TF_MAX_DF,\n",
    "                        max_features=TF_MAX_FEATURES)\n",
    "\n",
    "X_tfidf = tfvec.fit_transform(df['filtered_text'])\n",
    "feature_names = np.array(tfvec.get_feature_names_out())\n",
    "\n",
    "# Also build count matrix for chi2 (chi2 expects counts)\n",
    "cntvec = CountVectorizer(stop_words='english',\n",
    "                         ngram_range=TF_NGRAM_RANGE,\n",
    "                         min_df=TF_MIN_DF,\n",
    "                         max_df=TF_MAX_DF,\n",
    "                         max_features=TF_MAX_FEATURES,\n",
    "                         vocabulary=tfvec.vocabulary_)  # same vocab\n",
    "X_count = cntvec.fit_transform(df['filtered_text'])\n",
    "\n",
    "# ---------------------------\n",
    "# OPTIONAL: Clustering for exploration\n",
    "# ---------------------------\n",
    "km = KMeans(n_clusters=N_CLUSTERS, random_state=RANDOM_STATE, n_init=10)\n",
    "df['cluster'] = km.fit_predict(X_tfidf)\n",
    "\n",
    "# Compute top phrases per cluster (mean TF-IDF)\n",
    "def top_terms_for_cluster(cluster_id, top_n=20, min_weight=0.003):\n",
    "    idx = np.where(df['cluster'] == cluster_id)[0]\n",
    "    if idx.size == 0:\n",
    "        return []\n",
    "    mean = X_tfidf[idx].mean(axis=0).A1\n",
    "    order = np.argsort(mean)[::-1]\n",
    "    selected = []\n",
    "    for i in order:\n",
    "        if mean[i] < min_weight:\n",
    "            break\n",
    "        term = feature_names[i]\n",
    "        # drop if contains tokens entirely in NOISE_TOKENS\n",
    "        toks = term.split()\n",
    "        if all(tok in NOISE_TOKENS for tok in toks):\n",
    "            continue\n",
    "        selected.append((term, mean[i]))\n",
    "        if len(selected) >= top_n:\n",
    "            break\n",
    "    return selected\n",
    "\n",
    "# quick print of cluster top phrases (optional)\n",
    "for c in range(N_CLUSTERS):\n",
    "    t = top_terms_for_cluster(c, top_n=10)\n",
    "    print(f\"\\nCluster {c} top (sample): {[x[0] for x in t]}\")\n",
    "\n",
    "# ---------------------------\n",
    "# STEP: Find discriminative ngrams associated with positive amount (proxy for finding)\n",
    "# ---------------------------\n",
    "# create a noisy target: has_amount\n",
    "df['has_amount'] = df['overpayment_amount'].fillna(0) > 0\n",
    "\n",
    "# run chi2 to find terms correlated with has_amount\n",
    "chi2_stats, pvals = chi2(X_count, df['has_amount'].astype(int))\n",
    "chi2_df = pd.DataFrame({\n",
    "    'term': feature_names,\n",
    "    'chi2': chi2_stats,\n",
    "    'pval': pvals\n",
    "}).sort_values('chi2', ascending=False)\n",
    "\n",
    "# filter top candidates: also ensure term frequency in corpus >= SEED_MIN_COUNT\n",
    "occ = np.asarray(X_count.sum(axis=0)).ravel()\n",
    "chi2_df['count'] = occ[ [tfvec.vocabulary_[t] for t in chi2_df['term']] ]\n",
    "# keep strong signals\n",
    "candidate_finding_terms = chi2_df[(chi2_df['chi2'] >= MIN_CHI2_SCORE) & (chi2_df['count'] >= SEED_MIN_COUNT)]['term'].tolist()[:CHI2_TOP_K]\n",
    "\n",
    "print(f\"\\n{len(candidate_finding_terms)} candidate 'finding' phrases from chi2 (amount-associated). Example:\", candidate_finding_terms[:20])\n",
    "\n",
    "# ---------------------------\n",
    "# STEP: Define explicit high-precision patterns for cancelled / nofinding (regex list)\n",
    "# ---------------------------\n",
    "CANCEL_PATTERNS = [\n",
    "    r'\\bcancelled\\b', r'\\bcanceled\\b', r'\\bvoided\\b', r'\\bwithdrawn\\b', r'\\brescinded\\b',\n",
    "    r'\\bclosed without action\\b', r'\\bclose without action\\b'\n",
    "]\n",
    "NOFIND_PATTERNS = [\n",
    "    r'\\bno (?:finding|findings|overpayment|issue)\\b', r'\\bno discrepancies\\b', r'\\bvalid charge\\b',\n",
    "    r'\\bpaid correctly\\b', r'\\bnot an overpayment\\b', r'\\bno case to answer\\b'\n",
    "]\n",
    "\n",
    "CANCEL_RE = re.compile('|'.join(CANCEL_PATTERNS), flags=re.I)\n",
    "NOFIND_RE = re.compile('|'.join(NOFIND_PATTERNS), flags=re.I)\n",
    "\n",
    "# also define a small manual list of strong finding seed terms\n",
    "MANUAL_FINDING_TERMS = [\n",
    "    'overpayment', 'refund', 'recoup', 'recoupment', 'recover overpayment',\n",
    "    'amount owed', 'refund request', 'duplicate claim', 'duplicate payment', 'audit finding'\n",
    "]\n",
    "\n",
    "# combine manual + chi2 candidates (dedup)\n",
    "FINDING_SEED_TERMS = list(dict.fromkeys(MANUAL_FINDING_TERMS + candidate_finding_terms))\n",
    "\n",
    "print(\"\\nSome final candidate finding seeds (top 30):\", FINDING_SEED_TERMS[:30])\n",
    "\n",
    "# ---------------------------\n",
    "# Create weak (seed) labels per row (high precision rules)\n",
    "# Priority: cancelled > finding > nofinding\n",
    "# ---------------------------\n",
    "def seed_label_for_row(text):\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return None\n",
    "    if CANCEL_RE.search(text):\n",
    "        return 'cancelled_seed'\n",
    "    if any(re.search(r'\\b' + re.escape(t) + r'\\b', text) for t in FINDING_SEED_TERMS):\n",
    "        return 'finding_seed'\n",
    "    if NOFIND_RE.search(text):\n",
    "        return 'nofind_seed'\n",
    "    return None\n",
    "\n",
    "df['seed_label'] = df['filtered_text'].apply(seed_label_for_row)\n",
    "print(\"\\nSeed label counts:\\n\", df['seed_label'].value_counts(dropna=False))\n",
    "\n",
    "# ---------------------------\n",
    "# Prepare supervised training set (rows that got seed labels)\n",
    "# ---------------------------\n",
    "seed_df = df[df['seed_label'].notnull()].copy()\n",
    "# map seed names to short classes\n",
    "seed_map = {'finding_seed':'finding', 'nofind_seed':'nofinding', 'cancelled_seed':'cancelled'}\n",
    "seed_df['y'] = seed_df['seed_label'].map(seed_map)\n",
    "\n",
    "# If too few seed rows for any class, you might want to hand-label or relax seeds\n",
    "print(\"\\nSeed training counts:\\n\", seed_df['y'].value_counts())\n",
    "\n",
    "# If we have at least some training rows, train classifier\n",
    "if seed_df.shape[0] < 50:\n",
    "    print(\"WARNING: Very few seed-labeled rows (<50). Consider hand-labeling a small set to bootstrap.\")\n",
    "else:\n",
    "    # features: TF-IDF + scaled overpayment_amount (numeric)\n",
    "    # get row indices for seed rows\n",
    "    seed_idx = seed_df.index.values\n",
    "    X_seed_tfidf = X_tfidf[seed_idx]\n",
    "    # numeric feature\n",
    "    amount = df.loc[seed_idx, 'overpayment_amount'].fillna(0).values.reshape(-1,1)\n",
    "    scaler = StandardScaler()\n",
    "    amount_scaled = scaler.fit_transform(np.log1p(amount))\n",
    "    # combine sparse + dense -> sparse by hstack\n",
    "    X_seed = hstack([X_seed_tfidf, csr_matrix(amount_scaled)])\n",
    "    # train/test split\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_seed, seed_df['y'].values, test_size=0.2, random_state=RANDOM_STATE, stratify=seed_df['y'].values)\n",
    "    # classifier\n",
    "    clf = LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga', multi_class='multinomial', random_state=RANDOM_STATE)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    print(\"\\nTrained classifier on seed labels.\")\n",
    "    # evaluate\n",
    "    y_pred = clf.predict(X_val)\n",
    "    print(\"\\nValidation classification report:\")\n",
    "    print(classification_report(y_val, y_pred, digits=3))\n",
    "    # show per-class precision/recall\n",
    "    prfs = precision_recall_fscore_support(y_val, y_pred, labels=['finding','nofinding','cancelled'])\n",
    "    print(\"Precision,Recall,F1 by class (finding,nofinding,cancelled):\")\n",
    "    print(np.round(prfs[0],3), np.round(prfs[1],3), np.round(prfs[2],3))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Predict on full dataset\n",
    "    # ---------------------------\n",
    "    # prepare full feature matrix (tfidf + amount_scaled)\n",
    "    amount_all = df['overpayment_amount'].fillna(0).values.reshape(-1,1)\n",
    "    amount_all_scaled = scaler.transform(np.log1p(amount_all))\n",
    "    X_all = hstack([X_tfidf, csr_matrix(amount_all_scaled)])\n",
    "    probs = clf.predict_proba(X_all)\n",
    "    pred = clf.classes_[np.argmax(probs, axis=1)]\n",
    "    pred_prob = np.max(probs, axis=1)\n",
    "    df['pred_label_model'] = pred\n",
    "    df['pred_prob'] = pred_prob\n",
    "    # apply thresholding\n",
    "    df['final_label'] = np.where(df['pred_prob'] >= PROB_THRESHOLD, df['pred_label_model'], 'unknown')\n",
    "\n",
    "    # ---------------------------\n",
    "    # Evidence extraction per row (seed match or feature contributions)\n",
    "    # ---------------------------\n",
    "    # helper to get top contributing ngrams for a row toward a class\n",
    "    feature_count = X_tfidf.shape[1]\n",
    "    coef = clf.coef_   # shape (n_classes, n_features + 1) last column is numeric amount feature\n",
    "    classes = clf.classes_\n",
    "    # note: coef shape columns = X_all.shape[1] -> TF features + 1 numeric feature\n",
    "    def top_contributing_ngrams_for_row(row_idx, top_k=3):\n",
    "        # row tfidf vector\n",
    "        row_vec = X_tfidf[row_idx]\n",
    "        # get nonzero indices and values\n",
    "        row_coo = row_vec.tocoo()\n",
    "        cols = row_coo.col\n",
    "        vals = row_coo.data\n",
    "        if len(cols) == 0:\n",
    "            return []\n",
    "        # class predicted\n",
    "        cl = df.loc[row_idx, 'pred_label_model']\n",
    "        cl_idx = list(classes).index(cl)\n",
    "        # coefficients for TF features only (exclude last amount column)\n",
    "        coef_cl = coef[cl_idx][:feature_count]\n",
    "        # compute contributions per nonzero ngram: coef * tfidf\n",
    "        contribs = vals * coef_cl[cols]\n",
    "        # pick top positive contributions\n",
    "        order = np.argsort(contribs)[::-1]\n",
    "        top = []\n",
    "        for ind in order[:top_k]:\n",
    "            if contribs[ind] <= 0:\n",
    "                continue\n",
    "            fn = feature_names[cols[ind]]\n",
    "            top.append(fn)\n",
    "        return top\n",
    "\n",
    "    # Build evidence column\n",
    "    def build_evidence_for_row(i):\n",
    "        text = df.at[i, 'filtered_text']\n",
    "        # seed match first\n",
    "        seed_matches = []\n",
    "        if CANCEL_RE.search(text):\n",
    "            seed_matches.append('cancelled_seed_match')\n",
    "        # finding seeds\n",
    "        for s in FINDING_SEED_TERMS:\n",
    "            if re.search(r'\\b' + re.escape(s) + r'\\b', text):\n",
    "                seed_matches.append(s)\n",
    "        for s in NOFIND_PATTERNS:\n",
    "            # NOFIND_PATTERNS var is a list of regex strings; we already compiled NOFIND_RE above\n",
    "            pass\n",
    "        if seed_matches:\n",
    "            return '; '.join(seed_matches)\n",
    "        # else fallback: top contributing ngrams\n",
    "        top_ngrams = top_contributing_ngrams_for_row(i, top_k=4)\n",
    "        if top_ngrams:\n",
    "            return '; '.join(top_ngrams)\n",
    "        # fallback to cluster terms\n",
    "        cl = df.at[i, 'cluster']\n",
    "        cl_terms = [t for t,_ in top_terms_for_cluster(cl, top_n=6)]\n",
    "        return '; '.join(cl_terms[:3])\n",
    "\n",
    "    # vectorized: but for clarity just loop (ok for up to few 100k rows)\n",
    "    evidences = []\n",
    "    for i in range(len(df)):\n",
    "        evidences.append(build_evidence_for_row(i))\n",
    "    df['evidence_phrases'] = evidences\n",
    "\n",
    "    # ---------------------------\n",
    "    # Save results\n",
    "    # ---------------------------\n",
    "    out_cols = list(df.columns)  # keep all columns\n",
    "    df.to_csv(\"claims_with_model_labels_and_evidence.csv\", index=False)\n",
    "    print(\"\\nSaved results to claims_with_model_labels_and_evidence.csv\")\n",
    "\n",
    "    # OPTIONAL: show cluster-level summaries combining language + amount + predicted label distribution\n",
    "    for c in range(N_CLUSTERS):\n",
    "        sub = df[df['cluster'] == c]\n",
    "        print(\"\\n--- Cluster\", c, \"size\", len(sub), \"---\")\n",
    "        # show top strong terms using earlier function\n",
    "        top_c = top_terms_for_cluster(c, top_n=12)\n",
    "        print(\"Top phrases:\", [t for t,_ in top_c])\n",
    "        if 'overpayment_amount' in df.columns:\n",
    "            pct_pos = (sub['overpayment_amount'].fillna(0) > 0).mean() * 100\n",
    "            print(f\"% with positive overpayment_amount: {pct_pos:.1f}%\")\n",
    "        print(\"Predicted label counts:\\n\", sub['final_label'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e687d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# ===================================\n",
    "# Load and prepare merged text\n",
    "# ===================================\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# ===================================\n",
    "# Step 1: Clean and remove generic terms\n",
    "# ===================================\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"nan\", \"request\", \"letter\", \"sent\", \"medical\", \"record\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # remove numbers/special chars\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# ===================================\n",
    "# Step 2: Vectorize with n-grams\n",
    "# ===================================\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# ===================================\n",
    "# Step 3: Cluster\n",
    "# ===================================\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# ===================================\n",
    "# Step 4: Top terms per cluster (remove overlaps)\n",
    "# ===================================\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    return remove_redundant_phrases(raw_terms)\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(n_clusters)}\n",
    "\n",
    "# ===================================\n",
    "# Step 5: Evidence column\n",
    "# ===================================\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ===================================\n",
    "# Step 6: Rule-based label suggestion\n",
    "# ===================================\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\", \"audit findings\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_set = set(evidence)\n",
    "    if any(p in ev_set for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_set for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_set for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)), axis=1\n",
    ")\n",
    "\n",
    "# ===================================\n",
    "# Step 7: Save results\n",
    "# ===================================\n",
    "df.to_csv(\"claims_with_labels.csv\", index=False)\n",
    "print(\"Saved to claims_with_labels.csv\")\n",
    "\n",
    "# ===================================\n",
    "# Step 8: Extra view - samples and top terms\n",
    "# ===================================\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    print(\"Top terms:\", cluster_terms[c])\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(SAMPLES_PER_CLUSTER)\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "\n",
    "# ===================================\n",
    "# Step 9: Final review DataFrame\n",
    "# ===================================\n",
    "review_df = df.copy()\n",
    "review_df[\"evidence_terms_str\"] = review_df[\"evidence_terms\"].apply(lambda x: \"; \".join(x))\n",
    "review_df.to_csv(\"claims_cluster_review.csv\", index=False)\n",
    "review_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# High-accuracy pipeline (single cell)\n",
    "# - seed mining (chi2) + manual seed list\n",
    "# - weak supervision -> classifier (TF-IDF + overpayment_amount)\n",
    "# - evidence extraction (seed match OR top contributing ngrams)\n",
    "# - cluster diagnostics + top phrases per cluster\n",
    "# - final CSVs: detailed outputs for review\n",
    "# -------------------------------\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG - tune these for your data\n",
    "# -------------------------------\n",
    "N_CLUSTERS = 6                 # for exploration only\n",
    "TF_MIN_DF = 6                  # lower -> more phrases, higher -> more reliable\n",
    "TF_MAX_DF = 0.85\n",
    "TF_NGRAM_RANGE = (1, 3)        # unigrams..trigrams\n",
    "TF_MAX_FEATURES = 30000\n",
    "CHI2_TOP_K = 500\n",
    "MIN_CHI2_SCORE = 8.0\n",
    "SEED_MIN_COUNT = 15\n",
    "PROB_THRESHOLD = 0.80          # model confidence threshold to auto-accept label\n",
    "AMOUNT_OVERRIDE_THRESHOLD = 0  # if amount > this and no contradicting evidence, treat as signal\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# -------------------------------\n",
    "# 0) LOAD DATA - put your path here\n",
    "# -------------------------------\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "# Must include: communication_notes, free_flow_opt_note, overpayment_amount, claim_number (optional)\n",
    "if 'df' not in globals():\n",
    "    raise RuntimeError(\"Please load your DataFrame into the variable `df` before running this cell (e.g. df = pd.read_csv(...)).\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Merge fields and basic preprocess\n",
    "# -------------------------------\n",
    "def preprocess_text(s):\n",
    "    if pd.isna(s) or s is None:\n",
    "        return \"\"\n",
    "    text = str(s).lower()\n",
    "    # remove emails, urls, short id patterns like n19878, numeric tokens (we'll rely on amount separately)\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\d{2,}\\b', ' ', text)   # small letter+digits combos\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    # keep letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['merged_text'] = (df.get('communication_notes', pd.Series('', index=df.index)).fillna('') + ' ' +\n",
    "                     df.get('free_flow_opt_note', pd.Series('', index=df.index)).fillna(''))\n",
    "df['merged_text'] = df['merged_text'].apply(preprocess_text)\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Token-level noise filtering (customize REMOVE_TOKENS)\n",
    "# -------------------------------\n",
    "REMOVE_TOKENS = set([\n",
    "    'nan','md','drg','hrp','ovptrxid','claimid','omniclaim', \n",
    "    'please','thank','regards','sent','request','requested','note','chart','order','record','per'\n",
    "])\n",
    "MIN_TOKEN_LEN = 3\n",
    "WHITELIST_SHORT = {'rpa','offset','icd'}  # keep meaningful short tokens if needed\n",
    "\n",
    "def filter_tokens(txt):\n",
    "    toks = []\n",
    "    for t in txt.split():\n",
    "        if t in REMOVE_TOKENS:\n",
    "            continue\n",
    "        if len(t) < MIN_TOKEN_LEN and t not in WHITELIST_SHORT:\n",
    "            continue\n",
    "        toks.append(t)\n",
    "    return ' '.join(toks)\n",
    "\n",
    "df['filtered_text'] = df['merged_text'].apply(filter_tokens)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) TF-IDF and Count vectorizers (same vocab)\n",
    "# -------------------------------\n",
    "tfidf = TfidfVectorizer(stop_words='english',\n",
    "                        ngram_range=TF_NGRAM_RANGE,\n",
    "                        min_df=TF_MIN_DF,\n",
    "                        max_df=TF_MAX_DF,\n",
    "                        max_features=TF_MAX_FEATURES)\n",
    "X_tfidf = tfidf.fit_transform(df['filtered_text'])\n",
    "feature_names = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "countvec = CountVectorizer(ngram_range=TF_NGRAM_RANGE, min_df=TF_MIN_DF,\n",
    "                           max_df=TF_MAX_DF, vocabulary=tfidf.vocabulary_)\n",
    "X_count = countvec.fit_transform(df['filtered_text'])\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Clustering for exploration (optional)\n",
    "# -------------------------------\n",
    "km = KMeans(n_clusters=N_CLUSTERS, random_state=RANDOM_STATE, n_init=10)\n",
    "df['cluster'] = km.fit_predict(X_tfidf)\n",
    "\n",
    "def top_terms_for_cluster(cluster_id, top_n=20, min_weight=0.002):\n",
    "    idx = np.where(df['cluster'] == cluster_id)[0]\n",
    "    if idx.size == 0:\n",
    "        return []\n",
    "    mean = X_tfidf[idx].mean(axis=0).A1\n",
    "    order = np.argsort(mean)[::-1]\n",
    "    out = []\n",
    "    for i in order:\n",
    "        if mean[i] < min_weight:\n",
    "            break\n",
    "        term = feature_names[i]\n",
    "        # ignore pure noise tokens\n",
    "        if all(tok in REMOVE_TOKENS for tok in term.split()):\n",
    "            continue\n",
    "        out.append((term, mean[i]))\n",
    "        if len(out) >= top_n:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "# print small sample for debug\n",
    "for c in range(min(N_CLUSTERS, 8)):\n",
    "    sample_terms = [t for t,_ in top_terms_for_cluster(c, top_n=8)]\n",
    "    print(f\"Cluster {c} sample top: {sample_terms}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Data-driven seed discovery (chi2) - find ngrams associated with positive overpayment\n",
    "# -------------------------------\n",
    "df['has_amount'] = df.get('overpayment_amount', pd.Series(0, index=df.index)).fillna(0) > 0\n",
    "chi2_stats, pvals = chi2(X_count, df['has_amount'].astype(int))\n",
    "chi2_df = pd.DataFrame({'term': feature_names, 'chi2': chi2_stats, 'pval': pvals})\n",
    "# attach counts\n",
    "counts = np.asarray(X_count.sum(axis=0)).ravel()\n",
    "chi2_df['count'] = counts\n",
    "chi2_df = chi2_df.sort_values('chi2', ascending=False)\n",
    "\n",
    "candidates = chi2_df[(chi2_df['chi2'] >= MIN_CHI2_SCORE) & (chi2_df['count'] >= SEED_MIN_COUNT)]['term'].tolist()[:CHI2_TOP_K]\n",
    "print(f\"Found {len(candidates)} candidate finding terms via chi2. Example slice: {candidates[:30]}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6) Manual seed lists (add domain phrases you know are strong)\n",
    "# -------------------------------\n",
    "MANUAL_FINDING = [\n",
    "    'overpayment', 'overpayment identified', 'overpayment recovered', 'refund request',\n",
    "    'recoup', 'recoupment', 'recover overpayment', 'audit finding', 'audit findings',\n",
    "    'duplicate claim', 'duplicate payment', 'amount owed', 'refund due', 'appeal upheld',\n",
    "    'appeal denied', 'dispute upheld', 'dispute denied'\n",
    "]\n",
    "# Combine chi2 + manual seeds (unique, ordered)\n",
    "finding_seeds = []\n",
    "for s in MANUAL_FINDING + candidates:\n",
    "    if s not in finding_seeds:\n",
    "        finding_seeds.append(s)\n",
    "\n",
    "# Cancellation & no-find regex phrases\n",
    "CANCEL_PATTERNS = [r'\\bcancelled\\b', r'\\bcanceled\\b', r'\\bwithdrawn\\b', r'\\bvoided\\b', r'\\brescinded\\b']\n",
    "NOFIND_PATTERNS = [r'\\bno (?:finding|findings|overpayment|issue)\\b', r'\\bno discrepancy', r'\\bvalid charge\\b', r'\\bpaid correctly\\b']\n",
    "CANCEL_RE = re.compile('|'.join(CANCEL_PATTERNS), flags=re.I)\n",
    "NOFIND_RE = re.compile('|'.join(NOFIND_PATTERNS), flags=re.I)\n",
    "\n",
    "print(\"\\nSample final finding seeds (top 40):\", finding_seeds[:40])\n",
    "\n",
    "# -------------------------------\n",
    "# 7) Build high-precision seed labels (weak labels)\n",
    "# Priority: cancelled_seed > finding_seed > nofind_seed\n",
    "# -------------------------------\n",
    "def seed_label_for_row(text):\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return None\n",
    "    if CANCEL_RE.search(text):\n",
    "        return 'cancelled_seed'\n",
    "    for s in finding_seeds:\n",
    "        # exact token-boundary match\n",
    "        if re.search(r'\\b' + re.escape(s) + r'\\b', text):\n",
    "            return 'finding_seed'\n",
    "    if NOFIND_RE.search(text):\n",
    "        return 'nofind_seed'\n",
    "    return None\n",
    "\n",
    "df['seed_label'] = df['filtered_text'].apply(seed_label_for_row)\n",
    "print(\"\\nSeed counts:\", df['seed_label'].value_counts(dropna=False))\n",
    "\n",
    "# -------------------------------\n",
    "# 8) Prepare training data (rows with seed labels)\n",
    "# -------------------------------\n",
    "seed_df = df[df['seed_label'].notnull()].copy()\n",
    "seed_df['y'] = seed_df['seed_label'].map({'finding_seed':'finding','nofind_seed':'nofinding','cancelled_seed':'cancelled'})\n",
    "\n",
    "print(\"\\nTraining counts from seeds:\\n\", seed_df['y'].value_counts())\n",
    "\n",
    "# If too few seeds, consider hand-labeling some rows (recommended)\n",
    "if seed_df.shape[0] < 100:\n",
    "    print(\"WARNING: fewer than 100 seed-labeled rows. Consider adding some manual labels for better accuracy.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9) Train classifier on TF-IDF + numeric overpayment\n",
    "# -------------------------------\n",
    "model_trained = False\n",
    "if seed_df.shape[0] >= 50:\n",
    "    seed_idx = seed_df.index.values\n",
    "    X_seed_tfidf = X_tfidf[seed_idx]\n",
    "    # numeric feature: log1p(amount)\n",
    "    amount_seed = df.loc[seed_idx, 'overpayment_amount'].fillna(0).values.reshape(-1,1)\n",
    "    scaler = StandardScaler()\n",
    "    amount_seed_scaled = scaler.fit_transform(np.log1p(amount_seed))\n",
    "    X_seed = hstack([X_seed_tfidf, csr_matrix(amount_seed_scaled)])\n",
    "    # split\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_seed, seed_df['y'].values, test_size=0.2, random_state=RANDOM_STATE, stratify=seed_df['y'].values)\n",
    "    clf = LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga', multi_class='multinomial', random_state=RANDOM_STATE)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    model_trained = True\n",
    "    # eval\n",
    "    y_pred = clf.predict(X_val)\n",
    "    print(\"\\nValidation report on seed-labeled holdout:\")\n",
    "    print(classification_report(y_val, y_pred, digits=3))\n",
    "\n",
    "# -------------------------------\n",
    "# 10) Score entire dataset with classifier + build final label logic\n",
    "# -------------------------------\n",
    "if model_trained:\n",
    "    amount_all = df['overpayment_amount'].fillna(0).values.reshape(-1,1)\n",
    "    amount_all_scaled = scaler.transform(np.log1p(amount_all))\n",
    "    X_all = hstack([X_tfidf, csr_matrix(amount_all_scaled)])\n",
    "    probs = clf.predict_proba(X_all)\n",
    "    pred = clf.classes_[np.argmax(probs, axis=1)]\n",
    "    pred_prob = np.max(probs, axis=1)\n",
    "    df['pred_label_model'] = pred\n",
    "    df['pred_prob'] = pred_prob\n",
    "\n",
    "    # Final label logic:\n",
    "    # - If cancel regex match -> cancelled\n",
    "    # - Else if seed finding matched -> finding\n",
    "    # - Else if model predicts with high prob -> model's label\n",
    "    # - Else if amount > AMOUNT_OVERRIDE_THRESHOLD -> finding\n",
    "    # - else unknown\n",
    "    def final_label_row(i):\n",
    "        text = df.at[i, 'filtered_text']\n",
    "        amt = df.at[i, 'overpayment_amount'] if not pd.isna(df.at[i, 'overpayment_amount']) else 0\n",
    "        # cancel explicit\n",
    "        if CANCEL_RE.search(text):\n",
    "            return 'cancelled'\n",
    "        # seed finding\n",
    "        if df.at[i, 'seed_label'] == 'finding_seed':\n",
    "            return 'finding'\n",
    "        # model confident\n",
    "        if df.at[i, 'pred_prob'] >= PROB_THRESHOLD:\n",
    "            return df.at[i, 'pred_label_model']\n",
    "        # amount tie-breaker\n",
    "        if amt > AMOUNT_OVERRIDE_THRESHOLD:\n",
    "            return 'finding'\n",
    "        return 'unknown'\n",
    "\n",
    "    df['final_label'] = [final_label_row(i) for i in range(len(df))]\n",
    "else:\n",
    "    print(\"Model not trained due to insufficient seeds; falling back to rule-only labeling.\")\n",
    "    # simple rule-only final label\n",
    "    def final_label_rule(i):\n",
    "        text = df.at[i, 'filtered_text']\n",
    "        amt = df.at[i, 'overpayment_amount'] if not pd.isna(df.at[i, 'overpayment_amount']) else 0\n",
    "        if CANCEL_RE.search(text):\n",
    "            return 'cancelled'\n",
    "        for s in finding_seeds:\n",
    "            if re.search(r'\\b' + re.escape(s) + r'\\b', text):\n",
    "                return 'finding'\n",
    "        if NOFIND_RE.search(text) and amt == 0:\n",
    "            return 'nofinding'\n",
    "        if amt > AMOUNT_OVERRIDE_THRESHOLD:\n",
    "            return 'finding'\n",
    "        return 'unknown'\n",
    "    df['final_label'] = [final_label_rule(i) for i in range(len(df))]\n",
    "\n",
    "# -------------------------------\n",
    "# 11) Evidence extraction per row (seed match or top contributing ngrams)\n",
    "# -------------------------------\n",
    "# compute feature_count\n",
    "feature_count = X_tfidf.shape[1]\n",
    "if model_trained:\n",
    "    coef = clf.coef_   # shape (n_classes, n_features + 1)\n",
    "    classes = clf.classes_\n",
    "else:\n",
    "    coef = None\n",
    "    classes = []\n",
    "\n",
    "def top_contrib_ngrams(row_idx, top_k=3):\n",
    "    if not model_trained:\n",
    "        return []\n",
    "    row_vec = X_tfidf[row_idx]\n",
    "    row_coo = row_vec.tocoo()\n",
    "    cols = row_coo.col\n",
    "    vals = row_coo.data\n",
    "    if len(cols) == 0:\n",
    "        return []\n",
    "    cl = df.at[row_idx, 'pred_label_model']\n",
    "    cl_idx = list(classes).index(cl)\n",
    "    coef_cl = coef[cl_idx][:feature_count]\n",
    "    contribs = vals * coef_cl[cols]\n",
    "    order = np.argsort(contribs)[::-1]\n",
    "    top = []\n",
    "    for ind in order:\n",
    "        if contribs[ind] <= 0:\n",
    "            continue\n",
    "        fn = feature_names[cols[ind]]\n",
    "        top.append(fn)\n",
    "        if len(top) >= top_k:\n",
    "            break\n",
    "    return top\n",
    "\n",
    "def build_evidence(i):\n",
    "    text = df.at[i, 'filtered_text']\n",
    "    # cancel seed\n",
    "    if CANCEL_RE.search(text):\n",
    "        return 'cancelled_seed'\n",
    "    # finding seed match\n",
    "    for s in finding_seeds:\n",
    "        if re.search(r'\\b' + re.escape(s) + r'\\b', text):\n",
    "            return s\n",
    "    # nofind regex\n",
    "    if NOFIND_RE.search(text):\n",
    "        return 'nofind_seed'\n",
    "    # model contributions fallback\n",
    "    contribs = top_contrib_ngrams(i, top_k=4)\n",
    "    if contribs:\n",
    "        return '; '.join(contribs)\n",
    "    # cluster terms fallback\n",
    "    cl = df.at[i, 'cluster']\n",
    "    cl_terms = [t for t,_ in top_terms_for_cluster(cl, top_n=6)]\n",
    "    return '; '.join(cl_terms[:3])\n",
    "\n",
    "df['evidence_phrases'] = [build_evidence(i) for i in range(len(df))]\n",
    "\n",
    "# -------------------------------\n",
    "# 12) Cluster-level diagnostics (top terms + amount distribution + predicted labels)\n",
    "# -------------------------------\n",
    "cluster_summary = []\n",
    "for c in range(N_CLUSTERS):\n",
    "    sub = df[df['cluster'] == c]\n",
    "    if len(sub) == 0:\n",
    "        continue\n",
    "    top = [t for t,_ in top_terms_for_cluster(c, top_n=15)]\n",
    "    pct_amount = (sub['overpayment_amount'].fillna(0) > 0).mean() * 100\n",
    "    label_counts = sub['final_label'].value_counts(normalize=True).to_dict()\n",
    "    cluster_summary.append({\n",
    "        'cluster': c,\n",
    "        'size': len(sub),\n",
    "        'top_phrases': top,\n",
    "        '%with_amount': pct_amount,\n",
    "        'label_distribution': label_counts\n",
    "    })\n",
    "    print(f\"\\nCluster {c} | size {len(sub)} | %with_amount {pct_amount:.1f}%\")\n",
    "    print(\"Top phrases:\", top[:12])\n",
    "    print(\"Label distribution:\", label_counts)\n",
    "\n",
    "# -------------------------------\n",
    "# 13) Final exports for review\n",
    "# -------------------------------\n",
    "# create review dataframe with clear columns\n",
    "review_cols = list(df.columns)  # keep all\n",
    "# create a compact DF for review (subset of columns)\n",
    "compact_cols = [\n",
    "    'final_label', 'pred_label_model' if model_trained else None, 'pred_prob' if model_trained else None,\n",
    "    'evidence_phrases', 'cluster', 'filtered_text', 'merged_text', 'overpayment_amount'\n",
    "]\n",
    "compact_cols = [c for c in compact_cols if c is not None]\n",
    "df_review = df[compact_cols + [c for c in df.columns if c not in compact_cols]]  # keep everything but ensure key columns up-front\n",
    "\n",
    "# save files\n",
    "df.to_csv(\"claims_full_results.csv\", index=False)\n",
    "df_review.to_csv(\"claims_review_compact.csv\", index=False)\n",
    "import json\n",
    "with open(\"cluster_summary.json\", \"w\") as f:\n",
    "    json.dump(cluster_summary, f, default=str)\n",
    "\n",
    "print(\"\\nSaved outputs:\")\n",
    "print(\" - claims_full_results.csv  (complete)\")\n",
    "print(\" - claims_review_compact.csv  (compact review)\")\n",
    "print(\" - cluster_summary.json      (cluster diagnostics)\")\n",
    "\n",
    "# -------------------------------\n",
    "# 14) Recommendations (printed)\n",
    "# -------------------------------\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"1) Inspect 'claims_review_compact.csv' and the 'cluster_summary.json' to validate top phrases and label distribution.\")\n",
    "print(\"2) If you can manually label 300-500 rows (high-quality), retrain the classifier on those labels â€” accuracy will usually jump substantially.\")\n",
    "print(\"3) Tweak TF_MIN_DF, MIN_CHI2_SCORE, PROB_THRESHOLD to trade recall/precision.\")\n",
    "print(\"4) If you want, I can provide a quick notebook cell to sample 'unknown' rows for manual labeling.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "###13\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# === Load data ===\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# === Step 1: Clean and remove generic terms ===\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"nan\", \"request\", \"letter\", \"sent\", \"medical\", \"record\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # remove numbers/special chars\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# === Step 2: Vectorize with n-grams ===\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# === Step 3: Cluster ===\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# === Step 4: Remove redundant phrases ===\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    return remove_redundant_phrases(raw_terms)\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(n_clusters)}\n",
    "\n",
    "# === Step 5: Evidence column ===\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Step 6: Aggregate by claim_id before labeling ===\n",
    "if \"claim_id\" in df.columns:\n",
    "    agg_funcs = {col: lambda x: ' | '.join(map(str, set(x.dropna()))) for col in df.columns if col not in [\"overpayment_amount\"]}\n",
    "    agg_funcs[\"overpayment_amount\"] = \"max\"  # take max amount if multiple lines\n",
    "    df = df.groupby(\"claim_id\", as_index=False).agg(agg_funcs)\n",
    "\n",
    "# === Step 7: Rule-based label suggestion ===\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\", \"appeal denied\", \"audit findings\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_text = \" \".join(evidence).lower()\n",
    "    if any(p in ev_text for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_text for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_text for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)), axis=1\n",
    ")\n",
    "\n",
    "# === Step 8: Extra business rules ===\n",
    "# If posted_date exists and claim already posted once with finding, mark others as nofinding\n",
    "if \"posted_date\" in df.columns and \"business_area\" in df.columns:\n",
    "    df = df.sort_values(\"posted_date\")\n",
    "    df[\"final_label\"] = df.groupby(\"claim_id\")[\"pattern_label\"].transform(\n",
    "        lambda x: \"finding\" if \"finding\" in x.values else x.iloc[0]\n",
    "    )\n",
    "else:\n",
    "    df[\"final_label\"] = df[\"pattern_label\"]\n",
    "\n",
    "# === Step 9: Save results ===\n",
    "df.to_csv(\"claims_with_labels.csv\", index=False)\n",
    "print(\"Saved to claims_with_labels.csv\")\n",
    "\n",
    "# === Step 10: Show clusters and sample rows ===\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top terms: {cluster_terms[c]}\")\n",
    "    sample_rows = df[df[\"cluster\"] == c].head(5)\n",
    "    print(sample_rows[[\"claim_id\", \"filtered_text\", \"evidence_terms\", \"final_label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe243ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic clusters\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# --------------------------\n",
    "# Load and prepare data\n",
    "# --------------------------\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "# Assumes you have columns:\n",
    "# claim_id, communication_notes, free_flow_opt_note, overpayment_amount, posted_date, business_area\n",
    "\n",
    "# Step 1: Merge text per claim_id (multiple lines -> single row)\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "df = df.groupby(\n",
    "    [\"claim_id\", \"overpayment_amount\", \"posted_date\", \"business_area\"], as_index=False\n",
    ").agg({\"merged_text\": \" \".join})\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: Text cleaning\n",
    "# --------------------------\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"nan\", \"request\", \"letter\", \"sent\", \"medical\", \"record\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # remove numbers/special chars\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: Vectorization\n",
    "# --------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),  # capture up to trigrams\n",
    "    min_df=3\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: Auto-pick n_clusters\n",
    "# --------------------------\n",
    "sil_scores = {}\n",
    "for k in range(3, 10):\n",
    "    km_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels_temp = km_temp.fit_predict(X)\n",
    "    if len(set(labels_temp)) > 1:\n",
    "        sil = silhouette_score(X, labels_temp)\n",
    "        sil_scores[k] = sil\n",
    "\n",
    "best_k = max(sil_scores, key=sil_scores.get)\n",
    "print(\"Silhouette scores:\", sil_scores)\n",
    "print(\"Best cluster count:\", best_k)\n",
    "\n",
    "# --------------------------\n",
    "# Step 5: Final clustering\n",
    "# --------------------------\n",
    "km = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# --------------------------\n",
    "# Step 6: Top terms per cluster\n",
    "# --------------------------\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    return remove_redundant_phrases(raw_terms)\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(best_k)}\n",
    "\n",
    "# --------------------------\n",
    "# Step 7: Evidence terms per claim\n",
    "# --------------------------\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Step 8: Label suggestion rules\n",
    "# --------------------------\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\", \"appeal denied\", \"audit findings\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_set = set(evidence)\n",
    "    if any(p in ev_set for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_set for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_set for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Step 9: Preview results\n",
    "# --------------------------\n",
    "print(\"\\n=== Top terms per cluster ===\")\n",
    "for c in range(best_k):\n",
    "    print(f\"\\nCluster {c} ({len(df[df['cluster']==c])} claims):\")\n",
    "    print(cluster_terms[c])\n",
    "\n",
    "print(\"\\n=== Sample claims per cluster ===\")\n",
    "SAMPLES_PER_CLUSTER = 3\n",
    "for c in range(best_k):\n",
    "    print(f\"\\n--- Cluster {c} ---\")\n",
    "    for note in df[df[\"cluster\"] == c][\"merged_text\"].head(SAMPLES_PER_CLUSTER):\n",
    "        print(\"-\", note)\n",
    "\n",
    "# --------------------------\n",
    "# Step 10: Save final data\n",
    "# --------------------------\n",
    "df.to_csv(\"claims_with_labels.csv\", index=False)\n",
    "print(\"\\nSaved to claims_with_labels.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
