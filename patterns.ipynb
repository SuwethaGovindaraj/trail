{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------------------------\n",
    "# Load your data\n",
    "# --------------------------\n",
    "# Replace 'claims.csv' with your file path\n",
    "df = pd.read_csv('claims.csv')\n",
    "\n",
    "# Merge notes\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" +\n",
    "    df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning\n",
    "# --------------------------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Vectorize & cluster\n",
    "# --------------------------\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# Choose cluster count (tweak as needed)\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "df['cluster'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Top terms per cluster\n",
    "# --------------------------\n",
    "import numpy as np\n",
    "\n",
    "def top_terms_for_cluster(cluster_id, top_n=15):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    return [(terms[i], mean_tfidf[i]) for i in top_idx]\n",
    "\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} (size {len(df[df['cluster']==c])}):\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"{term:20s} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Add evidence column: matched top terms from each note's cluster\n",
    "# --------------------------\n",
    "def find_matches_in_text(text, terms):\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', text):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "# Build dictionary of cluster -> top terms\n",
    "cluster_top_terms = {c: [t for t, _ in top_terms_for_cluster(c)] for c in range(n_clusters)}\n",
    "\n",
    "# Create column with matches\n",
    "df[\"matched_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"clean_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df[[\"merged_text\", \"cluster\", \"matched_terms\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Show sample notes per cluster\n",
    "# --------------------------\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(SAMPLES_PER_CLUSTER)\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Interactive cluster tagging\n",
    "# --------------------------\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top terms:\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"  {term:20s} {score:.4f}\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(\"Enter label for this cluster (finding/nofinding/cancelled/unknown): \").strip().lower()\n",
    "    cluster_labels[c] = label\n",
    "\n",
    "# Assign labels back to df\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "# Save labeled dataset\n",
    "df.to_csv('claims_cluster_tagged.csv', index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456de5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning function\n",
    "# --------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove IDs like n19878, abc123 (letters+digits combos)\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\d{3,}\\b', ' ', text)\n",
    "    # Remove years/numbers\n",
    "    text = re.sub(r'\\b\\d{2,4}\\b', ' ', text)\n",
    "    # Remove known meaningless codes (expand this list as you see them)\n",
    "    custom_stopwords = {'drg', 'hrp', 'etc', 'na'}\n",
    "    text = ' '.join(w for w in text.split() if w not in custom_stopwords)\n",
    "    # Remove special characters & extra spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --------------------------\n",
    "# Prepare data\n",
    "# --------------------------\n",
    "df['merged_text'] = (\n",
    "    df['communication_notes'].fillna('') + ' ' + df['free_flow_opt_note'].fillna('')\n",
    ").apply(clean_text)\n",
    "\n",
    "# --------------------------\n",
    "# TF-IDF with bigrams\n",
    "# --------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=0.85,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2)  # single words + 2-word phrases\n",
    ")\n",
    "X = vectorizer.fit_transform(df['merged_text'])\n",
    "\n",
    "# --------------------------\n",
    "# Cluster\n",
    "# --------------------------\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# --------------------------\n",
    "# Top terms per cluster\n",
    "# --------------------------\n",
    "def top_terms_for_cluster(cluster_id, top_n=15, min_score=0.01):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1]\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        if mean_tfidf[i] >= min_score:  # skip very low-weight terms\n",
    "            results.append((terms[i], mean_tfidf[i]))\n",
    "        if len(results) >= top_n:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "# --------------------------\n",
    "# Display results\n",
    "# --------------------------\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n{'='*50}\\nCluster {c} (size {len(df[df['cluster']==c])})\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"{term:25s} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02016215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning function\n",
    "# --------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\d{3,}\\b', ' ', text)   # IDs\n",
    "    text = re.sub(r'\\b\\d{2,4}\\b', ' ', text)            # years/numbers\n",
    "    custom_stopwords = {'drg', 'hrp', 'etc', 'na'}      # expand this\n",
    "    text = ' '.join(w for w in text.split() if w not in custom_stopwords)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)               # non-letters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --------------------------\n",
    "# Prepare data\n",
    "# --------------------------\n",
    "df['merged_text'] = (\n",
    "    df['communication_notes'].fillna('') + ' ' + df['free_flow_opt_note'].fillna('')\n",
    ").apply(clean_text)\n",
    "\n",
    "# --------------------------\n",
    "# TF-IDF with bigrams\n",
    "# --------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=0.85,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "X = vectorizer.fit_transform(df['merged_text'])\n",
    "\n",
    "# --------------------------\n",
    "# Cluster\n",
    "# --------------------------\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# --------------------------\n",
    "# Top terms per cluster\n",
    "# --------------------------\n",
    "def top_terms_for_cluster(cluster_id, top_n=15, min_score=0.01):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1]\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        if mean_tfidf[i] >= min_score:\n",
    "            results.append((terms[i], mean_tfidf[i]))\n",
    "        if len(results) >= top_n:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "# --------------------------\n",
    "# Add evidence column\n",
    "# --------------------------\n",
    "def find_matches_in_text(text, terms):\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', text):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "cluster_top_terms = {c: [t for t, _ in top_terms_for_cluster(c)] for c in range(n_clusters)}\n",
    "df[\"matched_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"merged_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Show sample notes per cluster\n",
    "# --------------------------\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(SAMPLES_PER_CLUSTER)\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "\n",
    "# --------------------------\n",
    "# Interactive cluster tagging\n",
    "# --------------------------\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top terms:\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"  {term:20s} {score:.4f}\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(\"Enter label for this cluster (finding/nofinding/cancelled/unknown): \").strip().lower()\n",
    "    cluster_labels[c] = label\n",
    "\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "df.to_csv('claims_cluster_tagged.csv', index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Imports\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# ============================\n",
    "# 2. Load your data\n",
    "# ============================\n",
    "# df = pd.read_csv(\"your_claims.csv\")  # Replace with your file\n",
    "# Example columns: communication_notes, free_flow_opt_note\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\").astype(str) + \" \" +\n",
    "    df[\"free_flow_opt_note\"].fillna(\"\").astype(str)\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. Clean text\n",
    "# ============================\n",
    "def clean_text(txt):\n",
    "    txt = str(txt).lower()\n",
    "    txt = re.sub(r\"\\bnan\\b\", \" \", txt)             # remove 'nan'\n",
    "    txt = re.sub(r\"[^a-z0-9\\s]\", \" \", txt)         # remove punctuation\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "df[\"clean_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# ============================\n",
    "# 4. TF-IDF with bigrams & trigrams\n",
    "# ============================\n",
    "# Keep default stopwords but allow important domain words\n",
    "custom_stopwords = text.ENGLISH_STOP_WORDS - {\n",
    "    \"letter\", \"request\", \"sent\", \"medical\", \"record\"\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stopwords,\n",
    "    ngram_range=(1, 3),        # unigrams, bigrams, trigrams\n",
    "    min_df=5                   # must appear in at least 5 docs\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# ============================\n",
    "# 5. KMeans clustering\n",
    "# ============================\n",
    "n_clusters = 5  # You can adjust this\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# ============================\n",
    "# 6. Function: Top phrases per cluster\n",
    "# ============================\n",
    "def top_terms_for_cluster(cluster_id, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    return [(terms[i], mean_tfidf[i]) for i in top_idx]\n",
    "\n",
    "# Show top terms per cluster\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} (size {len(df[df['cluster'] == c])}) ===\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"{term:40s} {score:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 7. Evidence phrase extraction\n",
    "# ============================\n",
    "def find_matches_in_text(text, terms):\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        # Match full term as phrase\n",
    "        if re.search(r\"\\b\" + re.escape(term) + r\"\\b\", text):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "# Dictionary: cluster â†’ top phrases\n",
    "cluster_top_terms = {\n",
    "    c: [t for t, _ in top_terms_for_cluster(c)]\n",
    "    for c in range(n_clusters)\n",
    "}\n",
    "\n",
    "# Create column with matched phrases\n",
    "df[\"matched_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"clean_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 8. Sample viewer per cluster\n",
    "# ============================\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    print(\"Top phrases:\", \", \".join(cluster_top_terms[c]))\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].sample(min(SAMPLES_PER_CLUSTER, len(df[df['cluster'] == c])))\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "\n",
    "# ============================\n",
    "# 9. Interactive labeling\n",
    "# ============================\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top phrases: {', '.join(cluster_top_terms[c][:10])}\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(\"Enter label for this cluster (finding/nofinding/cancelled/unknown): \").strip().lower()\n",
    "    cluster_labels[c] = label\n",
    "\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "# ============================\n",
    "# 10. Save\n",
    "# ============================\n",
    "df.to_csv(\"claims_cluster_tagged.csv\", index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08843a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Imports\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# ============================\n",
    "# 2. Load your data\n",
    "# ============================\n",
    "# df = pd.read_csv(\"your_claims.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\").astype(str) + \" \" +\n",
    "    df[\"free_flow_opt_note\"].fillna(\"\").astype(str)\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 3. Clean text\n",
    "# ============================\n",
    "def clean_text(txt):\n",
    "    txt = str(txt).lower()\n",
    "    txt = re.sub(r\"\\bnan\\b\", \" \", txt)  # remove 'nan'\n",
    "    txt = re.sub(r\"[^a-z0-9\\s]\", \" \", txt)  # remove punctuation\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "df[\"clean_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# ============================\n",
    "# 4. TF-IDF with bigrams/trigrams\n",
    "# ============================\n",
    "custom_stopwords = text.ENGLISH_STOP_WORDS - {\n",
    "    \"letter\", \"request\", \"sent\", \"medical\", \"record\"\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=custom_stopwords,\n",
    "    ngram_range=(1, 3),        # unigrams, bigrams, trigrams\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# ============================\n",
    "# 5. KMeans clustering\n",
    "# ============================\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# ============================\n",
    "# 6. Top phrases per cluster\n",
    "# ============================\n",
    "def top_terms_for_cluster(cluster_id, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    return [(terms[i], mean_tfidf[i]) for i in top_idx]\n",
    "\n",
    "cluster_top_terms = {\n",
    "    c: [t for t, _ in top_terms_for_cluster(c)]\n",
    "    for c in range(n_clusters)\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# 7. Evidence phrase extraction\n",
    "# ============================\n",
    "def find_matches_in_text(text, terms):\n",
    "    return [term for term in terms if re.search(r\"\\b\" + re.escape(term) + r\"\\b\", text)]\n",
    "\n",
    "df[\"matched_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"clean_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 8. Auto-suggest cluster labels\n",
    "# ============================\n",
    "FINDING_KEYWORDS = [\n",
    "    \"overpayment\", \"offset\", \"uncheck\", \"refund\", \"rpa\",\n",
    "    \"recoupment\", \"dispute\", \"recover\", \"repayment\"\n",
    "]\n",
    "NOFINDING_KEYWORDS = [\n",
    "    \"no findings\", \"no issue\", \"valid\", \"compliant\",\n",
    "    \"nothing found\", \"no error\"\n",
    "]\n",
    "CANCELLED_KEYWORDS = [\n",
    "    \"cancelled\", \"withdrawn\", \"closed without action\",\n",
    "    \"claim withdrawn\"\n",
    "]\n",
    "\n",
    "def auto_label_cluster(phrases):\n",
    "    joined = \" \".join(phrases).lower()\n",
    "    if any(kw in joined for kw in FINDING_KEYWORDS):\n",
    "        return \"finding\"\n",
    "    elif any(kw in joined for kw in NOFINDING_KEYWORDS):\n",
    "        return \"nofinding\"\n",
    "    elif any(kw in joined for kw in CANCELLED_KEYWORDS):\n",
    "        return \"cancelled\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    suggestion = auto_label_cluster(cluster_top_terms[c])\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    print(\"Top phrases:\", \", \".join(cluster_top_terms[c]))\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(f\"Suggested label: {suggestion} â€” Press Enter to accept or type new label: \").strip().lower()\n",
    "    cluster_labels[c] = label if label else suggestion\n",
    "\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "# ============================\n",
    "# 9. Save\n",
    "# ============================\n",
    "df.to_csv(\"claims_cluster_tagged.csv\", index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 1. Imports\n",
    "# =====================================\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# =====================================\n",
    "# 2. Load your dataset\n",
    "# =====================================\n",
    "# df = pd.read_csv(\"your_claims_data.csv\")  # Replace with your file path\n",
    "\n",
    "# Merge the relevant text columns into one\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# Basic cleaning: lowercase, remove special chars\n",
    "df[\"clean_text\"] = (\n",
    "    df[\"merged_text\"]\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9\\s]\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# =====================================\n",
    "# 3. Remove stopwords, codes, numbers\n",
    "# =====================================\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"ovptrxid\", \"nan\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}\n",
    "\n",
    "def clean_and_filter_tokens(text):\n",
    "    tokens = text.split()\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in REMOVE_TERMS:\n",
    "            continue\n",
    "        if token.isdigit():\n",
    "            continue\n",
    "        if len(token) < MIN_WORD_LEN and token.lower() not in WHITELIST:\n",
    "            continue\n",
    "        filtered.append(token)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"clean_text\"].apply(clean_and_filter_tokens)\n",
    "\n",
    "# =====================================\n",
    "# 4. TF-IDF Vectorization\n",
    "# =====================================\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),  # unigrams, bigrams, trigrams\n",
    "    min_df=5,            # drop rare terms\n",
    "    token_pattern=r'\\b[a-zA-Z][a-zA-Z]+\\b'\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "\n",
    "# =====================================\n",
    "# 5. Clustering\n",
    "# =====================================\n",
    "n_clusters = 5  # You can adjust\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# =====================================\n",
    "# 6. Function to get top terms per cluster\n",
    "# =====================================\n",
    "def top_terms_for_cluster(cluster_id, top_n=15):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    return [(terms[i], mean_tfidf[i]) for i in top_idx]\n",
    "\n",
    "# =====================================\n",
    "# 7. Print top terms\n",
    "# =====================================\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} (size {len(df[df['cluster']==c])}):\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"{term:40s} {score:.4f}\")\n",
    "\n",
    "# =====================================\n",
    "# 8. Add evidence_terms column\n",
    "# =====================================\n",
    "def find_matches_in_text(text, terms):\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', text):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "cluster_top_terms = {c: [t for t, _ in top_terms_for_cluster(c)] for c in range(n_clusters)}\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"filtered_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# =====================================\n",
    "# 9. Show sample notes per cluster\n",
    "# =====================================\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(SAMPLES_PER_CLUSTER)\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "\n",
    "# =====================================\n",
    "# 10. Interactive cluster tagging\n",
    "# =====================================\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top terms:\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"  {term:40s} {score:.4f}\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(\"Enter label for this cluster (finding/nofinding/cancelled/unknown): \").strip().lower()\n",
    "    cluster_labels[c] = label\n",
    "\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "# =====================================\n",
    "# 11. Save final labeled dataset\n",
    "# =====================================\n",
    "df.to_csv('claims_cluster_tagged.csv', index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ef97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# === Step 1: Clean and remove generic terms ===\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"nan\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # remove numbers/special chars\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# === Step 2: Vectorize with n-grams ===\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# === Step 3: Cluster ===\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# === Step 4: Top terms per cluster (remove overlaps) ===\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    return remove_redundant_phrases(raw_terms)\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(n_clusters)}\n",
    "\n",
    "# === Step 5: Evidence column ===\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Step 6: Rule-based label suggestion ===\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_set = set(evidence)\n",
    "    if any(p in ev_set for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_set for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_set for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)), axis=1\n",
    ")\n",
    "\n",
    "# === Step 7: Save results ===\n",
    "df.to_csv(\"claims_with_labels.csv\", index=False)\n",
    "print(\"Saved to claims_with_labels.csv\")\n",
    "\n",
    "# === Debug: Show clusters ===\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c}: {cluster_terms[c]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70039a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Load & merge text columns\n",
    "# =========================\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 1: Clean text\n",
    "# =========================\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"nan\", \"request\", \"letter\", \"sent\", \"medical\", \"record\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}  # short words you still want\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # keep only letters\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# =========================\n",
    "# Step 2: TF-IDF Vectorizer (1-3 grams)\n",
    "# =========================\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# =========================\n",
    "# Step 3: KMeans clustering\n",
    "# =========================\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# =========================\n",
    "# Step 4: Top terms per cluster (remove overlaps)\n",
    "# =========================\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    return remove_redundant_phrases(raw_terms)\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(n_clusters)}\n",
    "\n",
    "# =========================\n",
    "# Step 5: Evidence terms per claim\n",
    "# =========================\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 6: Rule-based label suggestion\n",
    "# =========================\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_set = set(evidence)\n",
    "    if any(p in ev_set for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_set for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_set for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)), axis=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 7: Save results\n",
    "# =========================\n",
    "df.to_csv(\"claims_with_labels.csv\", index=False)\n",
    "print(\"âœ… Saved to claims_with_labels.csv\")\n",
    "\n",
    "# =========================\n",
    "# Step 8: Show cluster summaries\n",
    "# =========================\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "\n",
    "for c in range(n_clusters):\n",
    "    cluster_df = df[df['cluster'] == c]\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸ“Œ Cluster {c} â€” size: {len(cluster_df)}\")\n",
    "    print(\"Top 20 terms/phrases:\")\n",
    "    for term in cluster_terms[c]:\n",
    "        print(f\"  {term}\")\n",
    "\n",
    "    # Overpayment stats\n",
    "    if \"overpayment_amount\" in df.columns:\n",
    "        pos_amount = (cluster_df[\"overpayment_amount\"] > 0).mean() * 100\n",
    "        print(f\"\\nðŸ’° % with positive overpayment_amount: {pos_amount:.1f}%\")\n",
    "\n",
    "    print(\"\\nSample claim notes:\")\n",
    "    sample_rows = cluster_df.sample(\n",
    "        min(SAMPLES_PER_CLUSTER, len(cluster_df)),\n",
    "        random_state=42\n",
    "    )\n",
    "    for note in sample_rows[\"merged_text\"]:\n",
    "        print(f\"- {note}\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 1: Clean text\n",
    "# =========================\n",
    "REMOVE_TERMS = {\n",
    "    \"md\", \"drg\", \"hrp\", \"nan\", \"request\", \"letter\", \"sent\", \"medical\", \"record\",\n",
    "    \"chart\", \"order\", \"verify\", \"reported\", \"please\", \"note\", \"per\"\n",
    "}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}  # short words you want to keep\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# =========================\n",
    "# Step 2: TF-IDF Vectorizer (1-3 grams)\n",
    "# =========================\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5,\n",
    "    max_df=0.8  # remove phrases appearing in >80% of claims\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# =========================\n",
    "# Step 3: KMeans clustering\n",
    "# =========================\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# =========================\n",
    "# Step 4: Top terms per cluster (remove overlaps)\n",
    "# =========================\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n * 2]  # take more, then filter\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    # remove admin-like meaningless phrases again\n",
    "    cleaned = [t for t in raw_terms if not any(stop in t.split() for stop in REMOVE_TERMS)]\n",
    "    return remove_redundant_phrases(cleaned)[:top_n]\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(n_clusters)}\n",
    "\n",
    "# =========================\n",
    "# Step 5: Evidence terms per claim\n",
    "# =========================\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 6: Rule-based label suggestion\n",
    "# =========================\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_set = set(evidence)\n",
    "    if any(p in ev_set for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_set for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_set for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)), axis=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 7: Create final descriptive column for clarity\n",
    "# =========================\n",
    "df[\"cluster_top_terms\"] = df[\"cluster\"].apply(lambda c: \", \".join(cluster_terms[c]))\n",
    "\n",
    "# =========================\n",
    "# Step 8: Save full enriched dataset\n",
    "# =========================\n",
    "df.to_csv(\"claims_with_clusters_and_labels.csv\", index=False)\n",
    "print(\"âœ… Saved to claims_with_clusters_and_labels.csv\")\n",
    "\n",
    "# =========================\n",
    "# Step 9: Show summaries\n",
    "# =========================\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "\n",
    "for c in range(n_clusters):\n",
    "    cluster_df = df[df['cluster'] == c]\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸ“Œ Cluster {c} â€” size: {len(cluster_df)}\")\n",
    "    print(\"Top 20 terms/phrases:\")\n",
    "    for term in cluster_terms[c]:\n",
    "        print(f\"  {term}\")\n",
    "\n",
    "    if \"overpayment_amount\" in df.columns:\n",
    "        pos_amount = (cluster_df[\"overpayment_amount\"] > 0).mean() * 100\n",
    "        print(f\"\\nðŸ’° % with positive overpayment_amount: {pos_amount:.1f}%\")\n",
    "\n",
    "    print(\"\\nSample claim notes:\")\n",
    "    sample_rows = cluster_df.sample(\n",
    "        min(SAMPLES_PER_CLUSTER, len(cluster_df)),\n",
    "        random_state=42\n",
    "    )\n",
    "    for note in sample_rows[\"merged_text\"]:\n",
    "        print(f\"- {note}\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52620e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load data\n",
    "# ----------------------------------------\n",
    "# df = pd.read_csv(\"claims_data.csv\")\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" + df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 1: Clean and remove generic terms\n",
    "# ----------------------------------------\n",
    "REMOVE_TERMS = {\"md\", \"drg\", \"hrp\", \"nan\", \"request\", \"letter\", \"sent\", \"medical\", \"record\"}\n",
    "MIN_WORD_LEN = 3\n",
    "WHITELIST = {\"rpa\", \"offset\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # remove numbers/special chars\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        if tok in REMOVE_TERMS:\n",
    "            continue\n",
    "        if len(tok) < MIN_WORD_LEN and tok not in WHITELIST:\n",
    "            continue\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"filtered_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 2: Vectorize with n-grams\n",
    "# ----------------------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"filtered_text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 3: Cluster\n",
    "# ----------------------------------------\n",
    "n_clusters = 5\n",
    "km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = km.fit_predict(X)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 4: Top terms per cluster (remove overlaps)\n",
    "# ----------------------------------------\n",
    "def remove_redundant_phrases(phrases):\n",
    "    phrases = sorted(phrases, key=len, reverse=True)\n",
    "    final = []\n",
    "    for p in phrases:\n",
    "        if not any(p in bigger for bigger in final if p != bigger):\n",
    "            final.append(p)\n",
    "    return final\n",
    "\n",
    "def top_terms_for_cluster(c, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    raw_terms = [terms[i] for i in top_idx]\n",
    "    return remove_redundant_phrases(raw_terms)\n",
    "\n",
    "cluster_terms = {c: top_terms_for_cluster(c) for c in range(n_clusters)}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 5: Evidence column\n",
    "# ----------------------------------------\n",
    "def find_matches(text, phrases):\n",
    "    found = []\n",
    "    for phrase in phrases:\n",
    "        if re.search(r'\\b' + re.escape(phrase) + r'\\b', text):\n",
    "            found.append(phrase)\n",
    "    return found\n",
    "\n",
    "df[\"evidence_terms\"] = df.apply(\n",
    "    lambda row: find_matches(row[\"filtered_text\"], cluster_terms[row[\"cluster\"]]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 6: Rule-based label suggestion\n",
    "# ----------------------------------------\n",
    "FINDING_PHRASES = {\"overpayment closing\", \"recover overpayment\", \"audit completed\"}\n",
    "NOFINDING_PHRASES = {\"no findings\", \"no overpayment\", \"reconsideration denied\"}\n",
    "CANCELLED_PHRASES = {\"withdrawn\", \"cancelled\", \"rescinded\"}\n",
    "\n",
    "def suggest_label(evidence, amount):\n",
    "    ev_set = set(evidence)\n",
    "    if any(p in ev_set for p in CANCELLED_PHRASES):\n",
    "        return \"cancelled\"\n",
    "    if any(p in ev_set for p in FINDING_PHRASES):\n",
    "        return \"finding\"\n",
    "    if any(p in ev_set for p in NOFINDING_PHRASES) and amount == 0:\n",
    "        return \"nofinding\"\n",
    "    if amount > 0:\n",
    "        return \"finding\"\n",
    "    return \"unknown\"\n",
    "\n",
    "df[\"pattern_label\"] = df.apply(\n",
    "    lambda r: suggest_label(r[\"evidence_terms\"], r.get(\"overpayment_amount\", 0)), axis=1\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 7: Save results\n",
    "# ----------------------------------------\n",
    "df.to_csv(\"claims_with_labels.csv\", index=False)\n",
    "print(\"Saved to claims_with_labels.csv\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 8: Debug - Show clusters\n",
    "# ----------------------------------------\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c}: {cluster_terms[c]}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# EXTRA VIEW: Filter top terms by minimum TF-IDF weight\n",
    "# ----------------------------------------\n",
    "MIN_TFIDF_WEIGHT = 0.05  # adjust threshold\n",
    "\n",
    "def top_terms_above_threshold(c, min_weight=MIN_TFIDF_WEIGHT, top_n=20):\n",
    "    idx = df[df[\"cluster\"] == c].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    mask = mean_tfidf >= min_weight\n",
    "    filtered_idx = np.where(mask)[0]\n",
    "    sorted_idx = filtered_idx[np.argsort(mean_tfidf[filtered_idx])[::-1]]\n",
    "    raw_terms = [terms[i] for i in sorted_idx]\n",
    "    cleaned = [t for t in raw_terms if not any(stop in t.split() for stop in REMOVE_TERMS)]\n",
    "    return remove_redundant_phrases(cleaned)[:top_n]\n",
    "\n",
    "strong_cluster_terms = {c: top_terms_above_threshold(c) for c in range(n_clusters)}\n",
    "\n",
    "print(\"\\n\\nðŸ“Š Strong Signal Top Terms per Cluster (TF-IDF >= {:.2f})\".format(MIN_TFIDF_WEIGHT))\n",
    "for c in range(n_clusters):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Cluster {c}\")\n",
    "    for term in strong_cluster_terms[c]:\n",
    "        print(f\"  {term}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 9: Add strong terms column for each row & save\n",
    "# ----------------------------------------\n",
    "df[\"strong_terms\"] = df[\"cluster\"].map(strong_cluster_terms)\n",
    "\n",
    "# Final export with everything\n",
    "df.to_csv(\"claims_with_labels_and_strong_terms.csv\", index=False)\n",
    "print(\"Saved to claims_with_labels_and_strong_terms.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
