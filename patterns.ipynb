{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim Notes Pattern Discovery & Cluster Tagging\n",
    "This notebook helps you:\n",
    "1. Load your claims data.\n",
    "2. Merge and clean the `communication_notes` and `free_flow_opt_note` columns.\n",
    "3. Cluster similar claim notes.\n",
    "4. View **top keywords** and **sample notes** for each cluster.\n",
    "5. Tag clusters as `finding`, `nofinding`, or `cancelled` interactively.\n",
    "\n",
    "You can then use these tags to create regex patterns or train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------------------------\n",
    "# Load your data\n",
    "# --------------------------\n",
    "# Replace 'claims.csv' with your file path\n",
    "df = pd.read_csv('claims.csv')\n",
    "\n",
    "# Merge notes\n",
    "df[\"merged_text\"] = (\n",
    "    df[\"communication_notes\"].fillna(\"\") + \" \" +\n",
    "    df[\"free_flow_opt_note\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning\n",
    "# --------------------------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"merged_text\"].apply(clean_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Vectorize & cluster\n",
    "# --------------------------\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# Choose cluster count (tweak as needed)\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "df['cluster'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Top terms per cluster\n",
    "# --------------------------\n",
    "import numpy as np\n",
    "\n",
    "def top_terms_for_cluster(cluster_id, top_n=15):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1][:top_n]\n",
    "    return [(terms[i], mean_tfidf[i]) for i in top_idx]\n",
    "\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} (size {len(df[df['cluster']==c])}):\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"{term:20s} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Add evidence column: matched top terms from each note's cluster\n",
    "# --------------------------\n",
    "def find_matches_in_text(text, terms):\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', text):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "# Build dictionary of cluster -> top terms\n",
    "cluster_top_terms = {c: [t for t, _ in top_terms_for_cluster(c)] for c in range(n_clusters)}\n",
    "\n",
    "# Create column with matches\n",
    "df[\"matched_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"clean_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df[[\"merged_text\", \"cluster\", \"matched_terms\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Show sample notes per cluster\n",
    "# --------------------------\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(SAMPLES_PER_CLUSTER)\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Interactive cluster tagging\n",
    "# --------------------------\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top terms:\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"  {term:20s} {score:.4f}\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(\"Enter label for this cluster (finding/nofinding/cancelled/unknown): \").strip().lower()\n",
    "    cluster_labels[c] = label\n",
    "\n",
    "# Assign labels back to df\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "# Save labeled dataset\n",
    "df.to_csv('claims_cluster_tagged.csv', index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456de5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning function\n",
    "# --------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove IDs like n19878, abc123 (letters+digits combos)\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\d{3,}\\b', ' ', text)\n",
    "    # Remove years/numbers\n",
    "    text = re.sub(r'\\b\\d{2,4}\\b', ' ', text)\n",
    "    # Remove known meaningless codes (expand this list as you see them)\n",
    "    custom_stopwords = {'drg', 'hrp', 'etc', 'na'}\n",
    "    text = ' '.join(w for w in text.split() if w not in custom_stopwords)\n",
    "    # Remove special characters & extra spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --------------------------\n",
    "# Prepare data\n",
    "# --------------------------\n",
    "df['merged_text'] = (\n",
    "    df['communication_notes'].fillna('') + ' ' + df['free_flow_opt_note'].fillna('')\n",
    ").apply(clean_text)\n",
    "\n",
    "# --------------------------\n",
    "# TF-IDF with bigrams\n",
    "# --------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=0.85,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2)  # single words + 2-word phrases\n",
    ")\n",
    "X = vectorizer.fit_transform(df['merged_text'])\n",
    "\n",
    "# --------------------------\n",
    "# Cluster\n",
    "# --------------------------\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# --------------------------\n",
    "# Top terms per cluster\n",
    "# --------------------------\n",
    "def top_terms_for_cluster(cluster_id, top_n=15, min_score=0.01):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1]\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        if mean_tfidf[i] >= min_score:  # skip very low-weight terms\n",
    "            results.append((terms[i], mean_tfidf[i]))\n",
    "        if len(results) >= top_n:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "# --------------------------\n",
    "# Display results\n",
    "# --------------------------\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n{'='*50}\\nCluster {c} (size {len(df[df['cluster']==c])})\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"{term:25s} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02016215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------------------------\n",
    "# Cleaning function\n",
    "# --------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\d{3,}\\b', ' ', text)   # IDs\n",
    "    text = re.sub(r'\\b\\d{2,4}\\b', ' ', text)            # years/numbers\n",
    "    custom_stopwords = {'drg', 'hrp', 'etc', 'na'}      # expand this\n",
    "    text = ' '.join(w for w in text.split() if w not in custom_stopwords)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)               # non-letters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --------------------------\n",
    "# Prepare data\n",
    "# --------------------------\n",
    "df['merged_text'] = (\n",
    "    df['communication_notes'].fillna('') + ' ' + df['free_flow_opt_note'].fillna('')\n",
    ").apply(clean_text)\n",
    "\n",
    "# --------------------------\n",
    "# TF-IDF with bigrams\n",
    "# --------------------------\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=0.85,\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "X = vectorizer.fit_transform(df['merged_text'])\n",
    "\n",
    "# --------------------------\n",
    "# Cluster\n",
    "# --------------------------\n",
    "n_clusters = 8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# --------------------------\n",
    "# Top terms per cluster\n",
    "# --------------------------\n",
    "def top_terms_for_cluster(cluster_id, top_n=15, min_score=0.01):\n",
    "    idx = df[df[\"cluster\"] == cluster_id].index\n",
    "    if len(idx) == 0:\n",
    "        return []\n",
    "    sub_matrix = X[idx]\n",
    "    mean_tfidf = sub_matrix.mean(axis=0).A1\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    top_idx = mean_tfidf.argsort()[::-1]\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        if mean_tfidf[i] >= min_score:\n",
    "            results.append((terms[i], mean_tfidf[i]))\n",
    "        if len(results) >= top_n:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "# --------------------------\n",
    "# Add evidence column\n",
    "# --------------------------\n",
    "def find_matches_in_text(text, terms):\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', text):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "cluster_top_terms = {c: [t for t, _ in top_terms_for_cluster(c)] for c in range(n_clusters)}\n",
    "df[\"matched_terms\"] = df.apply(\n",
    "    lambda row: \"; \".join(find_matches_in_text(row[\"merged_text\"], cluster_top_terms[row[\"cluster\"]])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Show sample notes per cluster\n",
    "# --------------------------\n",
    "SAMPLES_PER_CLUSTER = 5\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(SAMPLES_PER_CLUSTER)\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "\n",
    "# --------------------------\n",
    "# Interactive cluster tagging\n",
    "# --------------------------\n",
    "cluster_labels = {}\n",
    "for c in range(n_clusters):\n",
    "    print(f\"\\nCluster {c} top terms:\")\n",
    "    for term, score in top_terms_for_cluster(c):\n",
    "        print(f\"  {term:20s} {score:.4f}\")\n",
    "    sample_notes = df[df['cluster'] == c]['merged_text'].head(3)\n",
    "    print(\"\\nSample notes:\")\n",
    "    for note in sample_notes:\n",
    "        print(\"-\", note)\n",
    "    label = input(\"Enter label for this cluster (finding/nofinding/cancelled/unknown): \").strip().lower()\n",
    "    cluster_labels[c] = label\n",
    "\n",
    "df['pattern_label'] = df['cluster'].map(cluster_labels)\n",
    "df.to_csv('claims_cluster_tagged.csv', index=False)\n",
    "print(\"\\nSaved tagged data to claims_cluster_tagged.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
