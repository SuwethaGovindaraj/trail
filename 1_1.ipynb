{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b57127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_provider_combinations(df):\n",
    "\n",
    "    # Ensure columns exist\n",
    "    required_cols = [\n",
    "        \"providertaxid\", \"providernpi\", \n",
    "        \"sel_category\", \"paid_amount_bucket\",\n",
    "        \"audits\", \"findings\", \"nofindings\",\n",
    "        \"op_amount\", \"dispute_count\", \"overturn_count\"\n",
    "    ]\n",
    "\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0   # missing columns default to 0 so algorithm won't break\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 1: Derived metrics (dynamic)\n",
    "    # -------------------------------------------------------\n",
    "    df[\"hitrate\"] = df[\"findings\"] / df[\"audits\"].replace(0, np.nan)\n",
    "    df[\"dispute_rate\"] = df[\"dispute_count\"] / df[\"audits\"].replace(0, np.nan)\n",
    "    df[\"overturn_rate\"] = df[\"overturn_count\"] / df[\"findings\"].replace(0, np.nan)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 2: Dynamic scoring within each provider\n",
    "    # -------------------------------------------------------\n",
    "    def score_per_provider(group):\n",
    "\n",
    "        metric_cols = [\"hitrate\", \"op_amount\", \"audits\"]\n",
    "\n",
    "        # Compute z-scores only if variation exists\n",
    "        for col in metric_cols:\n",
    "            if group[col].nunique() > 1:\n",
    "                group[col + \"_z\"] = (group[col] - group[col].mean()) / group[col].std()\n",
    "            else:\n",
    "                group[col + \"_z\"] = 0\n",
    "\n",
    "        # Weighted final score (can adjust weights anytime)\n",
    "        group[\"score\"] = (\n",
    "            0.5 * group[\"hitrate_z\"] +\n",
    "            0.3 * group[\"op_amount_z\"] +\n",
    "            0.2 * group[\"audits_z\"]\n",
    "        )\n",
    "\n",
    "        return group\n",
    "\n",
    "    df = df.groupby([\"providertaxid\", \"providernpi\"], group_keys=False).apply(score_per_provider)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 3: Classification into buckets\n",
    "    # -------------------------------------------------------\n",
    "    def classify(row):\n",
    "        if row[\"audits\"] < 15:\n",
    "            return \"Insufficient Data – low audit sample\"\n",
    "        if row[\"score\"] > 0.7:\n",
    "            return \"High Performing\"\n",
    "        if 0.4 <= row[\"score\"] <= 0.7:\n",
    "            return \"Moderate Performing\"\n",
    "        if row[\"score\"] < 0.4:\n",
    "            return \"Low Performing\"\n",
    "    \n",
    "    df[\"performance_label\"] = df.apply(classify, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 4: Insight Generation\n",
    "    # -------------------------------------------------------\n",
    "    def create_insight(row):\n",
    "\n",
    "        if row[\"performance_label\"].startswith(\"Insufficient\"):\n",
    "            return (\n",
    "                f\"Only {row['audits']} audits — not enough data to judge performance \"\n",
    "                f\"in {row['sel_category']} – {row['paid_amount_bucket']}.\"\n",
    "            )\n",
    "\n",
    "        if row[\"performance_label\"] == \"High Performing\":\n",
    "            return (\n",
    "                f\"Strong performance in {row['sel_category']} – {row['paid_amount_bucket']}: \"\n",
    "                f\"Hitrate {row['hitrate']:.2%}, OP ${row['op_amount']:.2f}. \"\n",
    "                f\"Among the best categories for this provider.\"\n",
    "            )\n",
    "\n",
    "        if row[\"performance_label\"] == \"Moderate Performing\":\n",
    "            return (\n",
    "                f\"Average performance in {row['sel_category']} – {row['paid_amount_bucket']}. \"\n",
    "                f\"Hitrate and OP are close to provider's overall average.\"\n",
    "            )\n",
    "\n",
    "        if row[\"performance_label\"] == \"Low Performing\":\n",
    "            return (\n",
    "                f\"Poor performance in {row['sel_category']} – {row['paid_amount_bucket']}: \"\n",
    "                f\"Hitrate {row['hitrate']:.2%}, OP ${row['op_amount']:.2f}. \"\n",
    "                f\"Much lower than other categories for this provider. \"\n",
    "                f\"Recommended for deprioritization/exclusion.\"\n",
    "            )\n",
    "\n",
    "    df[\"insight\"] = df.apply(create_insight, axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def evaluate_provider_performance(df):\n",
    "    # Compute basic metrics\n",
    "    df['hitrate'] = df['findings'] / df['audits']\n",
    "    df['op_per_audit'] = df['overpayment'] / df['audits']\n",
    "    df['dispute_rate'] = df['disputes'] / df['audits']\n",
    "    df['overturn_rate'] = df['overturns'] / df['audits']\n",
    "\n",
    "    # Remove combinations with <15 audits ONLY FROM EVALUATION\n",
    "    df['eligible'] = df['audits'] >= 15\n",
    "\n",
    "    # Standardize metrics provider-wise (TIN + NPI level)\n",
    "    def provider_zscores(x):\n",
    "        metrics = ['hitrate','op_per_audit','overturn_rate']\n",
    "        for m in metrics:\n",
    "            x[f'{m}_z'] = zscore(x[m], ddof=1) if x[m].nunique() > 1 else 0\n",
    "        return x\n",
    "\n",
    "    df = df.groupby(['providertaxid','providernpi']).apply(provider_zscores)\n",
    "\n",
    "    # Overall performance score (higher = better)\n",
    "    df['performance_score'] = (\n",
    "        df['hitrate_z'] * 0.5 +\n",
    "        df['op_per_audit_z'] * 0.3 +\n",
    "        df['overturn_rate_z'] * 0.2\n",
    "    )\n",
    "\n",
    "    # Insights (dynamic – provider-wise quartiles)\n",
    "    def assign_insight(x):\n",
    "        if len(x[x['eligible']]) == 0:\n",
    "            x['insight'] = \"Not enough audit volume to evaluate\"\n",
    "            return x\n",
    "\n",
    "        scores = x.loc[x['eligible'], 'performance_score']\n",
    "        q1, q3 = scores.quantile([0.25, 0.75])\n",
    "\n",
    "        def label(row):\n",
    "            if not row['eligible']:\n",
    "                return \"Low audits — excluded from evaluation\"\n",
    "\n",
    "            if row['performance_score'] >= q3:\n",
    "                return (\"Strong performer — This selection category & paid bucket \"\n",
    "                        \"consistently shows high hitrate, OP and overturn success \"\n",
    "                        \"compared to other combinations for this provider\")\n",
    "\n",
    "            elif row['performance_score'] <= q1:\n",
    "                return (\"Weak performer — This combination underperforms in hitrate, OP \"\n",
    "                        \"and overturn rate compared to other categories for the same provider\")\n",
    "\n",
    "            else:\n",
    "                return (\"Moderate performer — Performance is neither high nor low but \"\n",
    "                        \"should be monitored relative to other categories\")\n",
    "\n",
    "        x['insight'] = x.apply(label, axis=1)\n",
    "        return x\n",
    "\n",
    "    df = df.groupby(['providertaxid','providernpi']).apply(assign_insight)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. MAIN ALGORITHM — AUTO DETECTS WEAK / STRONG COMBINATIONS\n",
    "# -------------------------------------------------------------\n",
    "def evaluate_provider_performance(df):\n",
    "\n",
    "    # ---- Basic computed metrics ----\n",
    "    df['hitrate'] = df['findings'] / df['audits']\n",
    "    df['op_per_audit'] = df['overpayment'] / df['audits']\n",
    "    df['dispute_rate'] = df['disputes'] / df['audits']\n",
    "    df['overturn_rate'] = df['overturns'] / df['audits']\n",
    "\n",
    "    # ---- Exclude low audit volume (<15) from scoring ----\n",
    "    df['eligible'] = df['audits'] >= 15\n",
    "\n",
    "    # ---- Compute z-scores within each provider ----\n",
    "    def provider_zscores(x):\n",
    "        metrics = ['hitrate', 'op_per_audit', 'overturn_rate']\n",
    "        for m in metrics:\n",
    "            if x[m].nunique() > 1:\n",
    "                x[f'{m}_z'] = zscore(x[m], ddof=1)\n",
    "            else:\n",
    "                x[f'{m}_z'] = 0\n",
    "        return x\n",
    "\n",
    "    df = df.groupby(['providertaxid', 'providernpi']).apply(provider_zscores)\n",
    "\n",
    "    # ---- Weighted dynamic performance score ----\n",
    "    df['performance_score'] = (\n",
    "        df['hitrate_z'] * 0.5 +\n",
    "        df['op_per_audit_z'] * 0.3 +\n",
    "        df['overturn_rate_z'] * 0.2\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 2. Assign insight category based on provider-level distribution\n",
    "    # -------------------------------------------------------------\n",
    "    def assign_insight(x):\n",
    "        # If provider has zero eligible rows\n",
    "        if len(x[x['eligible']]) == 0:\n",
    "            x['insight'] = \"Not enough audit volume to evaluate\"\n",
    "            return x\n",
    "\n",
    "        eligible_scores = x.loc[x['eligible'], 'performance_score']\n",
    "        q1, q3 = eligible_scores.quantile([0.25, 0.75])\n",
    "\n",
    "        def label(row):\n",
    "            if not row['eligible']:\n",
    "                return \"Low audits (<15) — excluded from evaluation\"\n",
    "\n",
    "            if row['performance_score'] >= q3:\n",
    "                return (\n",
    "                    \"Strong performer — This category shows superior hitrate, \"\n",
    "                    \"higher overpayment recovery per audit, and better overturn success \"\n",
    "                    \"than other categories for this provider.\"\n",
    "                )\n",
    "\n",
    "            elif row['performance_score'] <= q1:\n",
    "                return (\n",
    "                    \"Weak performer — Underperforms in hitrate, overpayment recovery, \"\n",
    "                    \"and overturn rate compared to other categories of the same provider.\"\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                return (\n",
    "                    \"Moderate performer — Average performance; neither strong nor weak. \"\n",
    "                    \"Monitor for future trend.\"\n",
    "                )\n",
    "\n",
    "        x['insight'] = x.apply(label, axis=1)\n",
    "        return x\n",
    "\n",
    "    df = df.groupby(['providertaxid','providernpi']).apply(assign_insight)\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. REPORT GENERATOR — MANAGER FRIENDLY SUMMARY\n",
    "# -------------------------------------------------------------\n",
    "def generate_provider_report(df):\n",
    "\n",
    "    report_dict = {}\n",
    "    grouped = df.groupby(['providertaxid', 'providernpi'])\n",
    "\n",
    "    for (tin, npi), data in grouped:\n",
    "\n",
    "        # Only eligible categories (audits >=15)\n",
    "        eligible = data[data['eligible']]\n",
    "\n",
    "        if eligible.empty:\n",
    "            report_dict[(tin, npi)] = {\n",
    "                \"top_performers\": [],\n",
    "                \"weak_performers\": [],\n",
    "                \"summary\": f\"TIN {tin}, NPI {npi} cannot be evaluated (all categories have <15 audits).\"\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        top = eligible[eligible['insight'].str.contains(\"Strong performer\")]\n",
    "        weak = eligible[eligible['insight'].str.contains(\"Weak performer\")]\n",
    "\n",
    "        # Build readable summary\n",
    "        summary_lines = [\n",
    "            f\"Performance Summary for Provider (TIN: {tin}, NPI: {npi})\",\n",
    "            \"------------------------------------------------------------\"\n",
    "        ]\n",
    "\n",
    "        if not top.empty:\n",
    "            summary_lines.append(\n",
    "                f\"• {len(top)} strong performing categories — Higher hitrate, OP recovery \"\n",
    "                \"and overturn pattern compared to provider's other categories.\"\n",
    "            )\n",
    "        else:\n",
    "            summary_lines.append(\"• No strong performing categories detected.\")\n",
    "\n",
    "        if not weak.empty:\n",
    "            summary_lines.append(\n",
    "                f\"• {len(weak)} weak performing categories — These categories perform \"\n",
    "                \"significantly below the provider's other categories.\"\n",
    "            )\n",
    "        else:\n",
    "            summary_lines.append(\"• No underperforming categories detected.\")\n",
    "\n",
    "        summary_lines.append(f\"• Total categories evaluated: {len(eligible)}\")\n",
    "        summary_lines.append(\"\")\n",
    "\n",
    "        # Store report for this TIN+NPI\n",
    "        report_dict[(tin, npi)] = {\n",
    "            \"top_performers\": top[['sel_category','paid_amount_bucket','audits','hitrate','performance_score','insight']],\n",
    "            \"weak_performers\": weak[['sel_category','paid_amount_bucket','audits','hitrate','performance_score','insight']],\n",
    "            \"summary\": \"\\n\".join(summary_lines)\n",
    "        }\n",
    "\n",
    "    return report_dict\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. RUN THE FULL PIPELINE\n",
    "# -------------------------------------------------------------\n",
    "# df = your_dataframe  # <--- Replace with your real data\n",
    "\n",
    "# evaluated_df = evaluate_provider_performance(df)\n",
    "# provider_reports = generate_provider_report(evaluated_df)\n",
    "\n",
    "# NOW YOU CAN ACCESS:\n",
    "# provider_reports[(TIN, NPI)][\"summary\"]\n",
    "# provider_reports[(TIN, NPI)][\"top_performers\"]\n",
    "# provider_reports[(TIN, NPI)][\"weak_performers\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance Score =\n",
    "  0.40 × hitrate\n",
    "+ 0.35 × op_per_audit\n",
    "− 0.15 × dispute_on_find\n",
    "− 0.10 × overturn_on_find\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 1: METRIC CREATION\n",
    "# ----------------------------------------------------\n",
    "def build_metrics(df):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df['hitrate'] = df['findings'] / df['audits']\n",
    "    df['op_per_audit'] = df['op_amount'] / df['audits']\n",
    "    df['dispute_on_find'] = df['disputes'] / df['findings']\n",
    "    df['overturn_on_find'] = df['overturns'] / df['findings']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 2: STANDARDIZE WITHIN PEER GROUP\n",
    "# Peer = same model_band + sel_category\n",
    "# ----------------------------------------------------\n",
    "def standardize_peers(df):\n",
    "\n",
    "    metrics = [\n",
    "        'hitrate',\n",
    "        'op_per_audit',\n",
    "        'dispute_on_find',\n",
    "        'overturn_on_find'\n",
    "    ]\n",
    "\n",
    "    def zscore_group(x):\n",
    "        for m in metrics:\n",
    "            if x[m].nunique() > 1:\n",
    "                x[f'{m}_z'] = zscore(x[m], nan_policy='omit')\n",
    "            else:\n",
    "                x[f'{m}_z'] = 0\n",
    "        return x\n",
    "\n",
    "    return (\n",
    "        df.groupby(['model_band', 'sel_category'])\n",
    "          .apply(zscore_group)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 3: COMPOSITE PERFORMANCE SCORE (BUSINESS ALIGNED)\n",
    "# ----------------------------------------------------\n",
    "def compute_performance_score(df):\n",
    "\n",
    "    df['performance_score'] = (\n",
    "        df['hitrate_z'] * 0.40 +\n",
    "        df['op_per_audit_z'] * 0.35 -\n",
    "        df['dispute_on_find_z'] * 0.15 -\n",
    "        df['overturn_on_find_z'] * 0.10\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 4: DECISION LOGIC\n",
    "# ----------------------------------------------------\n",
    "def assign_decision(df):\n",
    "\n",
    "    df['decision'] = 'MONITOR'\n",
    "    df['insight'] = ''\n",
    "\n",
    "    def classify(group):\n",
    "\n",
    "        eligible = group[group['audits'] >= 15]\n",
    "\n",
    "        if eligible.empty:\n",
    "            group['decision'] = 'NOT EVALUATED'\n",
    "            group['insight'] = (\n",
    "                'Audit volume too low across this model band and '\n",
    "                'selection category to assess performance.'\n",
    "            )\n",
    "            return group\n",
    "\n",
    "        low_cut = eligible['performance_score'].quantile(0.25)\n",
    "        high_cut = eligible['performance_score'].quantile(0.75)\n",
    "\n",
    "        for idx, row in group.iterrows():\n",
    "\n",
    "            if row['audits'] < 15:\n",
    "                group.loc[idx, ['decision','insight']] = [\n",
    "                    'NOT EVALUATED',\n",
    "                    'Insufficient audits to reliably evaluate performance.'\n",
    "                ]\n",
    "\n",
    "            elif row['performance_score'] <= low_cut:\n",
    "                group.loc[idx, ['decision','insight']] = [\n",
    "                    'EXCLUDE',\n",
    "                    'Low hitrate and weak value recovery combined with higher '\n",
    "                    'dispute or overturn behavior relative to peers in the same '\n",
    "                    'model band and selection category.'\n",
    "                ]\n",
    "\n",
    "            elif row['performance_score'] >= high_cut:\n",
    "                group.loc[idx, ['decision','insight']] = [\n",
    "                    'KEEP',\n",
    "                    'Strong audit effectiveness with higher hitrate and overpayment '\n",
    "                    'recovery, and lower dispute/overturn rates compared to peers.'\n",
    "                ]\n",
    "\n",
    "            else:\n",
    "                group.loc[idx, ['decision','insight']] = [\n",
    "                    'MONITOR',\n",
    "                    'Performance is in line with peer combinations; continue monitoring.'\n",
    "                ]\n",
    "\n",
    "        return group\n",
    "\n",
    "    return (\n",
    "        df.groupby(['model_band', 'sel_category'])\n",
    "          .apply(classify)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 5: FULL PIPELINE\n",
    "# ----------------------------------------------------\n",
    "def evaluate_tin_npi_combinations(df):\n",
    "\n",
    "    df = build_metrics(df)\n",
    "    df = standardize_peers(df)\n",
    "    df = compute_performance_score(df)\n",
    "    df = assign_decision(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# RUN\n",
    "# ----------------------------------------------------\n",
    "# df = your_dataframe\n",
    "# result = evaluate_tin_npi_combinations(df)\n",
    "\n",
    "# FINAL OUTPUTS:\n",
    "# result[result['decision'] == 'EXCLUDE']\n",
    "# result[result['decision'] == 'KEEP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c460276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 1: METRICS\n",
    "# ----------------------------------------------------\n",
    "def create_metrics(df):\n",
    "\n",
    "    df = df.copy()\n",
    "    df['hitrate'] = df['findings'] / df['audits']\n",
    "    df['op_per_audit'] = df['op_amount'] / df['audits']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 2: DYNAMIC LOW THRESHOLDS\n",
    "# ----------------------------------------------------\n",
    "def compute_thresholds(df):\n",
    "\n",
    "    def tag(group):\n",
    "\n",
    "        hr_cut = group['hitrate'].quantile(0.30)\n",
    "        op_cut = group['op_per_audit'].quantile(0.30)\n",
    "\n",
    "        group['low_hitrate'] = group['hitrate'] < hr_cut\n",
    "        group['low_op'] = group['op_per_audit'] < op_cut\n",
    "\n",
    "        return group\n",
    "\n",
    "    return (\n",
    "        df.groupby(['model_band', 'sel_category'])\n",
    "          .apply(tag)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 3: FINAL DECISION LOGIC\n",
    "# ----------------------------------------------------\n",
    "def assign_decision(df):\n",
    "\n",
    "    df['decision'] = 'MONITOR'\n",
    "    df['insight'] = ''\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        if row['audits'] < 15:\n",
    "            df.loc[idx, ['decision','insight']] = [\n",
    "                'NOT EVALUATED',\n",
    "                'Audit volume below 15; performance not statistically reliable.'\n",
    "            ]\n",
    "\n",
    "        elif row['low_hitrate'] and row['low_op']:\n",
    "            df.loc[idx, ['decision','insight']] = [\n",
    "                'EXCLUDE',\n",
    "                'Both hitrate and overpayment recovery are low compared to peer '\n",
    "                'combinations within the same model band and selection category.'\n",
    "            ]\n",
    "\n",
    "        elif not row['low_hitrate'] and not row['low_op']:\n",
    "            df.loc[idx, ['decision','insight']] = [\n",
    "                'KEEP',\n",
    "                'Strong performance with acceptable hitrate and value recovery '\n",
    "                'relative to peers.'\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            df.loc[idx, ['decision','insight']] = [\n",
    "                'MONITOR',\n",
    "                'Mixed performance — either hitrate or value recovery is acceptable; '\n",
    "                'continue monitoring.'\n",
    "            ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 4: FULL PIPELINE\n",
    "# ----------------------------------------------------\n",
    "def evaluate_combinations(df):\n",
    "\n",
    "    df = create_metrics(df)\n",
    "    df = compute_thresholds(df)\n",
    "    df = assign_decision(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# RUN\n",
    "# ----------------------------------------------------\n",
    "# df = your_dataframe\n",
    "# result = evaluate_combinations(df)\n",
    "\n",
    "# EXCLUSION LIST:\n",
    "# result[result['decision'] == 'EXCLUDE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fixed leakage\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 1: METRIC CREATION\n",
    "# ----------------------------------------------------\n",
    "def create_metrics(df):\n",
    "\n",
    "    df = df.copy()\n",
    "    df['hitrate'] = df['findings'] / df['audits']\n",
    "    df['op_per_audit'] = df['op_amount'] / df['audits']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 2: COMPUTE DYNAMIC LOW THRESHOLDS\n",
    "# ----------------------------------------------------\n",
    "def compute_thresholds(df):\n",
    "\n",
    "    def tag(group):\n",
    "\n",
    "        # Exclude zero rows when computing percentiles\n",
    "        hr_nonzero = group.loc[group['hitrate'] > 0, 'hitrate']\n",
    "        op_nonzero = group.loc[group['op_per_audit'] > 0, 'op_per_audit']\n",
    "\n",
    "        hr_cut = hr_nonzero.quantile(0.30) if not hr_nonzero.empty else 0\n",
    "        op_cut = op_nonzero.quantile(0.30) if not op_nonzero.empty else 0\n",
    "\n",
    "        group['low_hitrate'] = group['hitrate'] <= hr_cut\n",
    "        group['low_op'] = group['op_per_audit'] <= op_cut\n",
    "\n",
    "        return group\n",
    "\n",
    "    return (\n",
    "        df.groupby(['model_band', 'sel_category'])\n",
    "          .apply(tag)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 3: FINAL DECISION LOGIC (NO LEAKAGE)\n",
    "# ----------------------------------------------------\n",
    "def assign_decision(df):\n",
    "\n",
    "    df['decision'] = ''\n",
    "    df['insight'] = ''\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        # 1️⃣ Low audit gate\n",
    "        if row['audits'] < 15:\n",
    "            df.loc[idx, ['decision','insight']] = [\n",
    "                'NOT EVALUATED',\n",
    "                'Audit volume below 15; results are not statistically reliable.'\n",
    "            ]\n",
    "\n",
    "        # 2️⃣ HARD FAIL — ZERO VALUE\n",
    "        elif row['hitrate'] == 0 and row['op_amount'] == 0:\n",
    "            df.loc[idx, ['decision','insight']] = [\n",
    "                'EXCLUDE',\n",
    "                'Zero findings and zero overpayment recovery indicate no audit value '\n",
    "                'for this model band and selection category.'\n",
    "            ]\n",
    "\n",
    "        # 3️⃣ RELATIVE UNDERPERFORMANCE\n",
    "        elif row['low_hitrate'] and row['low_op']:\n",
    "            df.loc[idx, ['decision','insight']] = [\n",
    "                'EXCLUDE',\n",
    "                'Both hitrate and overpayment recovery are low compared to peer '\n",
    "                'combinations within the same model band and selection category.'\n",
    "            ]\n",
    "\n",
    "        # 4️⃣ STRONG SIGNAL\n",
    "        elif not row['low_hitrate'] or not row['low_op']:\n",
    "            df.loc[idx, ['decision','insight']] = [\n",
    "                'KEEP',\n",
    "                'This combination demonstrates acceptable audit effectiveness or '\n",
    "                'financial recovery compared to peers.'\n",
    "            ]\n",
    "\n",
    "        # 5️⃣ SAFETY NET\n",
    "        else:\n",
    "            df.loc[idx, ['decision','insight']] = [\n",
    "                'MONITOR',\n",
    "                'Performance signals are mixed; continue monitoring.'\n",
    "            ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 4: FULL PIPELINE\n",
    "# ----------------------------------------------------\n",
    "def evaluate_combinations(df):\n",
    "\n",
    "    df = create_metrics(df)\n",
    "    df = compute_thresholds(df)\n",
    "    df = assign_decision(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# RUN\n",
    "# ----------------------------------------------------\n",
    "# df = your_dataframe\n",
    "# final_result = evaluate_combinations(df)\n",
    "\n",
    "# final_result[final_result['decision'] == 'EXCLUDE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "###latest\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 1: METRICS\n",
    "# ----------------------------------------------------\n",
    "def create_metrics(df):\n",
    "\n",
    "    df = df.copy()\n",
    "    df['hitrate'] = df['findings'] / df['audits']\n",
    "    df['op_per_audit'] = df['op_amount'] / df['audits']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 2: PEER AVERAGES (FOR SAFETY CHECK)\n",
    "# ----------------------------------------------------\n",
    "def add_peer_averages(df):\n",
    "\n",
    "    peer_avg = (\n",
    "        df.groupby(['model_band', 'sel_category'])\n",
    "          .agg(\n",
    "              peer_avg_hitrate=('hitrate', 'mean'),\n",
    "              peer_avg_op=('op_per_audit', 'mean')\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    return df.merge(peer_avg, on=['model_band', 'sel_category'], how='left')\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 3: EXCLUSION LOGIC (VERY STRICT)\n",
    "# ----------------------------------------------------\n",
    "def assign_exclusion(df):\n",
    "\n",
    "    df['decision'] = 'KEEP'\n",
    "    df['reason'] = ''\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        # 1️⃣ Ignore low audit volume\n",
    "        if row['audits'] < 15:\n",
    "            df.loc[idx, ['decision','reason']] = [\n",
    "                'NOT EVALUATED',\n",
    "                'Audit volume below 15; combination not assessed.'\n",
    "            ]\n",
    "            continue\n",
    "\n",
    "        # 2️⃣ Hard zero-value exclusion\n",
    "        if row['hitrate'] == 0 and row['op_amount'] == 0:\n",
    "            df.loc[idx, ['decision','reason']] = [\n",
    "                'EXCLUDE',\n",
    "                'Zero hitrate and zero overpayment recovery indicate no audit value.'\n",
    "            ]\n",
    "            continue\n",
    "\n",
    "        # 3️⃣ Very poor absolute performance\n",
    "        abs_poor = (\n",
    "            row['hitrate'] < 0.10 and\n",
    "            row['op_per_audit'] < 600\n",
    "        )\n",
    "\n",
    "        # 4️⃣ Relative underperformance vs peers\n",
    "        rel_poor = (\n",
    "            row['hitrate'] < 0.5 * row['peer_avg_hitrate'] and\n",
    "            row['op_per_audit'] < 0.5 * row['peer_avg_op']\n",
    "        )\n",
    "\n",
    "        if abs_poor and rel_poor:\n",
    "            df.loc[idx, ['decision','reason']] = [\n",
    "                'EXCLUDE',\n",
    "                'Extremely low hitrate and value recovery compared to both '\n",
    "                'business benchmarks and peer combinations.'\n",
    "            ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 4: FULL PIPELINE\n",
    "# ----------------------------------------------------\n",
    "def find_poor_combinations(df):\n",
    "\n",
    "    df = create_metrics(df)\n",
    "    df = add_peer_averages(df)\n",
    "    df = assign_exclusion(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# RUN\n",
    "# ----------------------------------------------------\n",
    "# df = your_dataframe\n",
    "# result = find_poor_combinations(df)\n",
    "\n",
    "# ONLY WORST COMBINATIONS:\n",
    "# result[result['decision'] == 'EXCLUDE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b811f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# BUSINESS CONSTANTS\n",
    "# -----------------------------\n",
    "MIN_AUDITS = 15\n",
    "LOW_HITRATE_ABS = 0.10              # 10%\n",
    "LOW_OP_PER_AUDIT = 600              # Loss floor\n",
    "PEER_RELATIVE_FACTOR = 0.50         # 50% of peer performance\n",
    "MAX_TOTAL_OP_TO_EXCLUDE = 100000    # Safety net\n",
    "\n",
    "# -----------------------------\n",
    "# INPUT DATAFRAME\n",
    "# Expected columns:\n",
    "# providertaxid, providernpi, sel_category, model_band,\n",
    "# audits, findings, nofindings, op_amount\n",
    "# -----------------------------\n",
    "df = df.copy()\n",
    "\n",
    "# -----------------------------\n",
    "# DERIVED METRICS\n",
    "# -----------------------------\n",
    "df['hitrate'] = df['findings'] / df['audits']\n",
    "df['op_per_audit'] = df['op_amount'] / df['audits']\n",
    "\n",
    "# -----------------------------\n",
    "# PEER BENCHMARKS (DYNAMIC)\n",
    "# Peers = same sel_category + model_band\n",
    "# -----------------------------\n",
    "peer_stats = (\n",
    "    df[df['audits'] >= MIN_AUDITS]\n",
    "    .groupby(['sel_category', 'model_band'])\n",
    "    .agg(\n",
    "        peer_avg_hitrate=('hitrate', 'mean'),\n",
    "        peer_avg_op_per_audit=('op_per_audit', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df = df.merge(peer_stats, on=['sel_category', 'model_band'], how='left')\n",
    "\n",
    "# -----------------------------\n",
    "# CORE CLASSIFICATION LOGIC\n",
    "# -----------------------------\n",
    "def classify_row(row):\n",
    "\n",
    "    # Rule 0: insufficient data\n",
    "    if row['audits'] < MIN_AUDITS:\n",
    "        return 'INSUFFICIENT_DATA'\n",
    "\n",
    "    # Rule 1: absolute zero value\n",
    "    if row['hitrate'] == 0 and row['op_amount'] == 0:\n",
    "        return 'EXCLUDE'\n",
    "\n",
    "    # Dynamic peer thresholds\n",
    "    hitrate_peer_threshold = row['peer_avg_hitrate'] * PEER_RELATIVE_FACTOR\n",
    "    op_peer_threshold = row['peer_avg_op_per_audit'] * PEER_RELATIVE_FACTOR\n",
    "\n",
    "    # Absolute poor checks\n",
    "    abs_poor_hitrate = row['hitrate'] < LOW_HITRATE_ABS\n",
    "    abs_poor_op = row['op_per_audit'] < LOW_OP_PER_AUDIT\n",
    "\n",
    "    # Relative poor checks\n",
    "    rel_poor_hitrate = row['hitrate'] < hitrate_peer_threshold\n",
    "    rel_poor_op = row['op_per_audit'] < op_peer_threshold\n",
    "\n",
    "    # Safety net: protect high value\n",
    "    low_total_value = row['op_amount'] < MAX_TOTAL_OP_TO_EXCLUDE\n",
    "\n",
    "    # Final EXCLUDE (strict, no leakage)\n",
    "    if (\n",
    "        abs_poor_hitrate and\n",
    "        abs_poor_op and\n",
    "        rel_poor_hitrate and\n",
    "        rel_poor_op and\n",
    "        low_total_value\n",
    "    ):\n",
    "        return 'EXCLUDE'\n",
    "\n",
    "    # Borderline\n",
    "    if abs_poor_hitrate or abs_poor_op:\n",
    "        return 'MODERATE'\n",
    "\n",
    "    return 'KEEP'\n",
    "\n",
    "df['decision'] = df.apply(classify_row, axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# BUSINESS INSIGHT TEXT\n",
    "# -----------------------------\n",
    "def generate_insight(row):\n",
    "\n",
    "    if row['decision'] == 'INSUFFICIENT_DATA':\n",
    "        return (\n",
    "            \"Audit volume under this selection category and model band is below \"\n",
    "            \"the minimum threshold, so performance cannot be reliably assessed.\"\n",
    "        )\n",
    "\n",
    "    if row['decision'] == 'EXCLUDE':\n",
    "        return (\n",
    "            \"This Tin–NPI combination performs significantly worse than peers \"\n",
    "            \"within this selection category and model band. Both hitrate and \"\n",
    "            \"overpayment yield are materially low, resulting in poor ROI. \"\n",
    "            \"Audit effort here is unlikely to generate meaningful value.\"\n",
    "        )\n",
    "\n",
    "    if row['decision'] == 'MODERATE':\n",
    "        return (\n",
    "            \"Performance is below optimal benchmarks for this selection category \"\n",
    "            \"and model band. Some value exists, but efficiency is weaker than peers. \"\n",
    "            \"This combination should be deprioritized but monitored.\"\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        \"This Tin–NPI combination performs in line with or better than peer \"\n",
    "        \"benchmarks within this selection category and model band and should \"\n",
    "        \"continue to be included.\"\n",
    "    )\n",
    "\n",
    "df['insight'] = df.apply(generate_insight, axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# OPTIONAL VALIDATION CHECK\n",
    "# Ensures excluded OP is low\n",
    "# -----------------------------\n",
    "exclude_validation = (\n",
    "    df[df['decision'] == 'EXCLUDE']\n",
    "    .agg(\n",
    "        total_audits=('audits', 'sum'),\n",
    "        total_op=('op_amount', 'sum'),\n",
    "        avg_hitrate=('hitrate', 'mean'),\n",
    "        avg_op_per_audit=('op_per_audit', 'mean')\n",
    "    )\n",
    ")\n",
    "\n",
    "print(exclude_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## another logic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "MIN_AUDITS = 15\n",
    "BOTTOM_PERCENTILE = 0.10     # worst 10%\n",
    "MAX_TOTAL_OP_TO_EXCLUDE = 100000\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "# -----------------------------\n",
    "# DERIVED METRICS\n",
    "# -----------------------------\n",
    "df['hitrate'] = df['findings'] / df['audits']\n",
    "df['op_per_audit'] = df['op_amount'] / df['audits']\n",
    "\n",
    "# -----------------------------\n",
    "# FILTER VALID DATA\n",
    "# -----------------------------\n",
    "df['decision'] = 'KEEP'\n",
    "df['insight'] = ''\n",
    "\n",
    "valid_df = df[df['audits'] >= MIN_AUDITS].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# PERCENTILE RANKING\n",
    "# -----------------------------\n",
    "valid_df['hitrate_pct'] = (\n",
    "    valid_df\n",
    "    .groupby(['sel_category', 'model_band'])['hitrate']\n",
    "    .rank(pct=True, method='average')\n",
    ")\n",
    "\n",
    "valid_df['op_pct'] = (\n",
    "    valid_df\n",
    "    .groupby(['sel_category', 'model_band'])['op_per_audit']\n",
    "    .rank(pct=True, method='average')\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# EXCLUSION LOGIC\n",
    "# -----------------------------\n",
    "exclude_mask = (\n",
    "    (\n",
    "        (valid_df['hitrate_pct'] <= BOTTOM_PERCENTILE) &\n",
    "        (valid_df['op_pct'] <= BOTTOM_PERCENTILE)\n",
    "    )\n",
    "    |\n",
    "    (\n",
    "        (valid_df['hitrate'] == 0) &\n",
    "        (valid_df['op_amount'] == 0)\n",
    "    )\n",
    ") & (valid_df['op_amount'] < MAX_TOTAL_OP_TO_EXCLUDE)\n",
    "\n",
    "valid_df.loc[exclude_mask, 'decision'] = 'EXCLUDE'\n",
    "\n",
    "# -----------------------------\n",
    "# MODERATE FLAG (OPTIONAL)\n",
    "# -----------------------------\n",
    "moderate_mask = (\n",
    "    (valid_df['decision'] == 'KEEP') &\n",
    "    (\n",
    "        (valid_df['hitrate_pct'] <= 0.25) |\n",
    "        (valid_df['op_pct'] <= 0.25)\n",
    "    )\n",
    ")\n",
    "\n",
    "valid_df.loc[moderate_mask, 'decision'] = 'MODERATE'\n",
    "\n",
    "# -----------------------------\n",
    "# MERGE BACK\n",
    "# -----------------------------\n",
    "df.update(valid_df[['decision']])\n",
    "\n",
    "# -----------------------------\n",
    "# INSIGHT TEXT\n",
    "# -----------------------------\n",
    "def build_insight(row):\n",
    "\n",
    "    if row['audits'] < MIN_AUDITS:\n",
    "        return \"Audit volume is too low under this selection category and model band to assess performance.\"\n",
    "\n",
    "    if row['decision'] == 'EXCLUDE':\n",
    "        return (\n",
    "            \"This Tin–NPI combination falls within the lowest performing group \"\n",
    "            \"for both hitrate and overpayment yield within this selection category \"\n",
    "            \"and model band. Audit effort here has consistently produced minimal value.\"\n",
    "        )\n",
    "\n",
    "    if row['decision'] == 'MODERATE':\n",
    "        return (\n",
    "            \"Performance is below peer benchmarks on either hitrate or overpayment \"\n",
    "            \"yield within this selection category and model band. This combination \"\n",
    "            \"should be monitored and deprioritized where possible.\"\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        \"This Tin–NPI combination performs in line with or better than peers \"\n",
    "        \"within this selection category and model band.\"\n",
    "    )\n",
    "\n",
    "df['insight'] = df.apply(build_insight, axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# VALIDATION CHECK\n",
    "# -----------------------------\n",
    "print(\n",
    "    df[df['decision'] == 'EXCLUDE']\n",
    "    .agg(\n",
    "        total_audits=('audits', 'sum'),\n",
    "        total_op=('op_amount', 'sum'),\n",
    "        avg_hitrate=('hitrate', 'mean'),\n",
    "        avg_op_per_audit=('op_per_audit', 'mean')\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2535c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##new\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "MIN_AUDITS = 15\n",
    "BOTTOM_PCT = 0.10\n",
    "LOW_OP_FLOOR = 600\n",
    "MAX_TOTAL_OP_TO_EXCLUDE = 100000\n",
    "\n",
    "GLOBAL_HITRATE_GOOD = 0.20\n",
    "GLOBAL_OP_PER_AUDIT_GOOD = 800\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "# -----------------------------\n",
    "# METRICS\n",
    "# -----------------------------\n",
    "df['hitrate'] = df['findings'] / df['audits']\n",
    "df['op_per_audit'] = df['op_amount'] / df['audits']\n",
    "\n",
    "# -----------------------------\n",
    "# STAGE 1: GLOBAL PROVIDER PERFORMANCE\n",
    "# -----------------------------\n",
    "provider_perf = (\n",
    "    df.groupby(['providertaxid', 'providernpi'])\n",
    "      .agg(\n",
    "          total_audits=('audits', 'sum'),\n",
    "          total_findings=('findings', 'sum'),\n",
    "          total_op=('op_amount', 'sum')\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "provider_perf['overall_hitrate'] = (\n",
    "    provider_perf['total_findings'] / provider_perf['total_audits']\n",
    ")\n",
    "provider_perf['overall_op_per_audit'] = (\n",
    "    provider_perf['total_op'] / provider_perf['total_audits']\n",
    ")\n",
    "\n",
    "provider_perf['provider_quality'] = np.where(\n",
    "    (provider_perf['overall_hitrate'] >= GLOBAL_HITRATE_GOOD) |\n",
    "    (provider_perf['overall_op_per_audit'] >= GLOBAL_OP_PER_AUDIT_GOOD),\n",
    "    'GOOD',\n",
    "    'WEAK'\n",
    ")\n",
    "\n",
    "df = df.merge(\n",
    "    provider_perf[['providertaxid', 'providernpi', 'provider_quality']],\n",
    "    on=['providertaxid', 'providernpi'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# STAGE 2: CATEGORY-LEVEL ANALYSIS\n",
    "# -----------------------------\n",
    "valid = df[df['audits'] >= MIN_AUDITS].copy()\n",
    "\n",
    "valid['hitrate_pct'] = (\n",
    "    valid\n",
    "    .groupby(['sel_category', 'model_band'])['hitrate']\n",
    "    .rank(pct=True)\n",
    ")\n",
    "\n",
    "valid['op_pct'] = (\n",
    "    valid\n",
    "    .groupby(['sel_category', 'model_band'])['op_per_audit']\n",
    "    .rank(pct=True)\n",
    ")\n",
    "\n",
    "# Default\n",
    "valid['decision'] = 'KEEP'\n",
    "\n",
    "# EXCLUDE ONLY FOR WEAK PROVIDERS\n",
    "exclude_mask = (\n",
    "    (valid['provider_quality'] == 'WEAK') &\n",
    "    (\n",
    "        (\n",
    "            (valid['hitrate'] == 0) &\n",
    "            (valid['op_amount'] == 0)\n",
    "        )\n",
    "        |\n",
    "        (\n",
    "            (valid['hitrate_pct'] <= BOTTOM_PCT) &\n",
    "            (valid['op_pct'] <= BOTTOM_PCT) &\n",
    "            (valid['op_per_audit'] < LOW_OP_FLOOR) &\n",
    "            (valid['op_amount'] < MAX_TOTAL_OP_TO_EXCLUDE)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "valid.loc[exclude_mask, 'decision'] = 'EXCLUDE'\n",
    "\n",
    "# MODERATE (optional)\n",
    "moderate_mask = (\n",
    "    (valid['decision'] == 'KEEP') &\n",
    "    (valid['provider_quality'] == 'WEAK')\n",
    ")\n",
    "\n",
    "valid.loc[moderate_mask, 'decision'] = 'MODERATE'\n",
    "\n",
    "# Merge back\n",
    "df['decision'] = 'INSUFFICIENT_DATA'\n",
    "df.loc[df['audits'] >= MIN_AUDITS, 'decision'] = valid['decision']\n",
    "\n",
    "# -----------------------------\n",
    "# INSIGHT\n",
    "# -----------------------------\n",
    "def insight(row):\n",
    "    if row['audits'] < MIN_AUDITS:\n",
    "        return \"Insufficient audits to assess this combination.\"\n",
    "\n",
    "    if row['decision'] == 'EXCLUDE':\n",
    "        return (\n",
    "            \"This combination delivers consistently low financial yield and \"\n",
    "            \"hitrate within this selection category and model band, and the \"\n",
    "            \"provider does not perform strongly overall. Excluding this \"\n",
    "            \"combination will reduce low-value audit effort.\"\n",
    "        )\n",
    "\n",
    "    if row['decision'] == 'MODERATE':\n",
    "        return (\n",
    "            \"The provider’s overall performance is weak, but this combination \"\n",
    "            \"does not meet strict exclusion criteria. Monitor and deprioritize.\"\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        \"This combination performs acceptably or the provider performs strongly \"\n",
    "        \"overall, so audit effort should be retained.\"\n",
    "    )\n",
    "\n",
    "df['insight'] = df.apply(insight, axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# VALIDATION\n",
    "# -----------------------------\n",
    "print(\n",
    "    df[df['decision'] == 'EXCLUDE']\n",
    "    .agg(\n",
    "        total_audits=('audits', 'sum'),\n",
    "        total_op=('op_amount', 'sum'),\n",
    "        avg_hitrate=('hitrate', 'mean'),\n",
    "        avg_op_per_audit=('op_per_audit', 'mean')\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
