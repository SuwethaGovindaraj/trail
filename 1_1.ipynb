{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b57127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_provider_combinations(df):\n",
    "\n",
    "    # Ensure columns exist\n",
    "    required_cols = [\n",
    "        \"providertaxid\", \"providernpi\", \n",
    "        \"sel_category\", \"paid_amount_bucket\",\n",
    "        \"audits\", \"findings\", \"nofindings\",\n",
    "        \"op_amount\", \"dispute_count\", \"overturn_count\"\n",
    "    ]\n",
    "\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0   # missing columns default to 0 so algorithm won't break\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 1: Derived metrics (dynamic)\n",
    "    # -------------------------------------------------------\n",
    "    df[\"hitrate\"] = df[\"findings\"] / df[\"audits\"].replace(0, np.nan)\n",
    "    df[\"dispute_rate\"] = df[\"dispute_count\"] / df[\"audits\"].replace(0, np.nan)\n",
    "    df[\"overturn_rate\"] = df[\"overturn_count\"] / df[\"findings\"].replace(0, np.nan)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 2: Dynamic scoring within each provider\n",
    "    # -------------------------------------------------------\n",
    "    def score_per_provider(group):\n",
    "\n",
    "        metric_cols = [\"hitrate\", \"op_amount\", \"audits\"]\n",
    "\n",
    "        # Compute z-scores only if variation exists\n",
    "        for col in metric_cols:\n",
    "            if group[col].nunique() > 1:\n",
    "                group[col + \"_z\"] = (group[col] - group[col].mean()) / group[col].std()\n",
    "            else:\n",
    "                group[col + \"_z\"] = 0\n",
    "\n",
    "        # Weighted final score (can adjust weights anytime)\n",
    "        group[\"score\"] = (\n",
    "            0.5 * group[\"hitrate_z\"] +\n",
    "            0.3 * group[\"op_amount_z\"] +\n",
    "            0.2 * group[\"audits_z\"]\n",
    "        )\n",
    "\n",
    "        return group\n",
    "\n",
    "    df = df.groupby([\"providertaxid\", \"providernpi\"], group_keys=False).apply(score_per_provider)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 3: Classification into buckets\n",
    "    # -------------------------------------------------------\n",
    "    def classify(row):\n",
    "        if row[\"audits\"] < 15:\n",
    "            return \"Insufficient Data – low audit sample\"\n",
    "        if row[\"score\"] > 0.7:\n",
    "            return \"High Performing\"\n",
    "        if 0.4 <= row[\"score\"] <= 0.7:\n",
    "            return \"Moderate Performing\"\n",
    "        if row[\"score\"] < 0.4:\n",
    "            return \"Low Performing\"\n",
    "    \n",
    "    df[\"performance_label\"] = df.apply(classify, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Step 4: Insight Generation\n",
    "    # -------------------------------------------------------\n",
    "    def create_insight(row):\n",
    "\n",
    "        if row[\"performance_label\"].startswith(\"Insufficient\"):\n",
    "            return (\n",
    "                f\"Only {row['audits']} audits — not enough data to judge performance \"\n",
    "                f\"in {row['sel_category']} – {row['paid_amount_bucket']}.\"\n",
    "            )\n",
    "\n",
    "        if row[\"performance_label\"] == \"High Performing\":\n",
    "            return (\n",
    "                f\"Strong performance in {row['sel_category']} – {row['paid_amount_bucket']}: \"\n",
    "                f\"Hitrate {row['hitrate']:.2%}, OP ${row['op_amount']:.2f}. \"\n",
    "                f\"Among the best categories for this provider.\"\n",
    "            )\n",
    "\n",
    "        if row[\"performance_label\"] == \"Moderate Performing\":\n",
    "            return (\n",
    "                f\"Average performance in {row['sel_category']} – {row['paid_amount_bucket']}. \"\n",
    "                f\"Hitrate and OP are close to provider's overall average.\"\n",
    "            )\n",
    "\n",
    "        if row[\"performance_label\"] == \"Low Performing\":\n",
    "            return (\n",
    "                f\"Poor performance in {row['sel_category']} – {row['paid_amount_bucket']}: \"\n",
    "                f\"Hitrate {row['hitrate']:.2%}, OP ${row['op_amount']:.2f}. \"\n",
    "                f\"Much lower than other categories for this provider. \"\n",
    "                f\"Recommended for deprioritization/exclusion.\"\n",
    "            )\n",
    "\n",
    "    df[\"insight\"] = df.apply(create_insight, axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def evaluate_provider_performance(df):\n",
    "    # Compute basic metrics\n",
    "    df['hitrate'] = df['findings'] / df['audits']\n",
    "    df['op_per_audit'] = df['overpayment'] / df['audits']\n",
    "    df['dispute_rate'] = df['disputes'] / df['audits']\n",
    "    df['overturn_rate'] = df['overturns'] / df['audits']\n",
    "\n",
    "    # Remove combinations with <15 audits ONLY FROM EVALUATION\n",
    "    df['eligible'] = df['audits'] >= 15\n",
    "\n",
    "    # Standardize metrics provider-wise (TIN + NPI level)\n",
    "    def provider_zscores(x):\n",
    "        metrics = ['hitrate','op_per_audit','overturn_rate']\n",
    "        for m in metrics:\n",
    "            x[f'{m}_z'] = zscore(x[m], ddof=1) if x[m].nunique() > 1 else 0\n",
    "        return x\n",
    "\n",
    "    df = df.groupby(['providertaxid','providernpi']).apply(provider_zscores)\n",
    "\n",
    "    # Overall performance score (higher = better)\n",
    "    df['performance_score'] = (\n",
    "        df['hitrate_z'] * 0.5 +\n",
    "        df['op_per_audit_z'] * 0.3 +\n",
    "        df['overturn_rate_z'] * 0.2\n",
    "    )\n",
    "\n",
    "    # Insights (dynamic – provider-wise quartiles)\n",
    "    def assign_insight(x):\n",
    "        if len(x[x['eligible']]) == 0:\n",
    "            x['insight'] = \"Not enough audit volume to evaluate\"\n",
    "            return x\n",
    "\n",
    "        scores = x.loc[x['eligible'], 'performance_score']\n",
    "        q1, q3 = scores.quantile([0.25, 0.75])\n",
    "\n",
    "        def label(row):\n",
    "            if not row['eligible']:\n",
    "                return \"Low audits — excluded from evaluation\"\n",
    "\n",
    "            if row['performance_score'] >= q3:\n",
    "                return (\"Strong performer — This selection category & paid bucket \"\n",
    "                        \"consistently shows high hitrate, OP and overturn success \"\n",
    "                        \"compared to other combinations for this provider\")\n",
    "\n",
    "            elif row['performance_score'] <= q1:\n",
    "                return (\"Weak performer — This combination underperforms in hitrate, OP \"\n",
    "                        \"and overturn rate compared to other categories for the same provider\")\n",
    "\n",
    "            else:\n",
    "                return (\"Moderate performer — Performance is neither high nor low but \"\n",
    "                        \"should be monitored relative to other categories\")\n",
    "\n",
    "        x['insight'] = x.apply(label, axis=1)\n",
    "        return x\n",
    "\n",
    "    df = df.groupby(['providertaxid','providernpi']).apply(assign_insight)\n",
    "\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
